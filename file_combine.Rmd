---
title: "Reliability"
output: html_notebook
---

```{r setup,warning=FALSE}
knitr::opts_knit$set(root.dir = 'H:/Projects/10000/10200/TS/Reliability/Data')

library(tidyverse)
library(lubridate)
library(gridExtra)
library(reshape2)
library(zoo)
library(rgdal)

```


We set the working directroy for the data files. Here are the sub-folders at this Network Location. 

```{r dir_list}
data.frame(file.info(dir()))
```

##Load Station Linear References

Load in the linear referenced Station values from the FileGeodatabase. The File Geodatabase is currently located on C Drive due to locking issue with File Geodatabases stored on the Network Drive. We will need the milepost for each station. We are importing the entire Station list and some will not have measure values. 

```{r I_94_EB_TT connect_geodb_read_station}
fgdb <- "C:/Users/dgallen/Desktop/Geos/MN Detectors/Congestion.gdb"
Station <- as.data.frame(readOGR(dsn=fgdb,layer="Station"))

Station <- Station[!is.na(Station$Measure),]

head(Station)
class(Station)
colnames(Station)
```

## Travel Time

###I-94 Eastbound Travel Time

We will focus on the Travel Time data for I_94 EB first. 


To load the Travel Time Data we first check that the files are present in the relevant folder. There are `r length(dir('./TICAS/I-94 EB'))` located in "./TICAS/I-94 EB".

```{r I_94_EB_TT Top_10_Files_dir}
dir('./TICAS/I-94 EB')[1:10]
```

Investigate the data structure of each TT File. Each file is for a specific day, with the file name giving the actual day. 

```{r I_94_EB_TT sample_TT_data}
head(read.csv('./TICAS/I-94 EB/TT_201501010000-201501020000.csv')[1:5,1:5])
```

Data is not in a tidy format. Rows do not appear to have names and the data is in a 'wide' format as opposed to the preferred 'long' format i.e each column is a new time. Also from opening some of the csv files manually using Excel we notice that the date column headers are not always correct. 

The file name states that the data is for Feb 9th to Feb 10th, but the column headers still state Jan 1st. However, the time are correct and are incrementing in 5 minutes intervals.

We will choose to ignore the column header to identify dates and we will import the filenames to give the true date reading. 
We will also fill down each row name to ensure that there are no blank station names. 

We will load the data into R and combine into one dataset. 

```{r I_94_EB_TT read_TT_files_bind,warning=FALSE}
### set the path for the I-94 EB data.
path<-'./TICAS/I-94 EB'

### list out all the csv files that start with TT_ and then have a number. This will avoid loading in the aggregated files.
list_csv <- dir(path=path,pattern = '^TT_\\d.')

##use lapply to read each file in the list, creating a large list object. 
###We set header to be false because as we established the headers cannot be trused for dates. This will also aid in combining the dataset later.
myfiles <- lapply(paste(path,'/',list_csv,sep=''),function(x)read.csv(x,header = FALSE,stringsAsFactors = FALSE))

## we not add a column to each list element for the corresponding filenames we loaded. 
##This will be used to create a true datetime attribute.
files <- mapply(cbind,myfiles,'filename' = list_csv,SIMPLIFY = F)

## bind all the elements of the list together.
I_94_EB_TT <- bind_rows(files)

## we need to get rid of the symbol in the station name column and turn it to NA.
symbol <- I_94_EB_TT$V1[4]

I_94_EB_TT$V1[I_94_EB_TT$V1==symbol] <- NA
```

We now clean the dataset and assign appropriate column names. 

```{r I_94_EB_TT fix_colnames_add_fields}
###I_94_EB_TT <- x

### Give the dataset the column name from the first row. This is only temporary.
colnames(I_94_EB_TT) <- I_94_EB_TT[1,]
colnames(I_94_EB_TT)[1] <- "STN_Name"
colnames(I_94_EB_TT)[292] <- 'filename'
colnames(I_94_EB_TT) <-trimws(colnames(I_94_EB_TT))


##remove the first row as it has now been transferred to the column names. 
I_94_EB_TT <- I_94_EB_TT[-1,]

## use dplyr piping to:
I_94_EB_TT <-  I_94_EB_TT %>%
  
  ##remove all of the headers that were read in orginially
  filter(I_94_EB_TT$`Accumulated Distance` != " Accumulated Distance")%>%
  
  ##Create new fields:
  ###fill in the blank rows with the previous name
  mutate(Name = paste(na.locf(STN_Name,na.rm=F),`Accumulated Distance`,seq=''),
         ###extract the station name from each row. The station name is included in parenthesis. 
         STN = gsub(".*\\((.*)\\).*","\\1",STN_Name),
         STN_ID = gsub(".*\\((.*)\\).*","\\1",Name),
         ## get the total lane number from the end of the station string.
         Lane = as.numeric(gsub(".*\\) ([^.]).*","\\1",Name)))%>%
  ##move the new columns to the front of the dataset
  select(Name,STN,STN_Name,STN_ID,Lane,filename,Average,everything())
  
###change the end of the day to 24:00 so it goes to the start of the next day
colnames(I_94_EB_TT) <- sub('00:00:00$','24:00:00',colnames(I_94_EB_TT))
  

```

The dataset is still in the wide format:

```{r I_94_EB_TT check_data_for_header_filter}
head(I_94_EB_TT[1:5,1:10])
table(I_94_EB_TT$`Accumulated Distance` == "Accumulated Distance")
```

Lets turn get a Time and Travel Time field effectively turning the dataset into the 'long format'. 

The melt function from reshape2 works well for this purpose.

```{r I_94_EB_TT melt_I94_EB}
I_94_EB_TT_melt <- melt(I_94_EB_TT,id.vars = c(1:8),variable.name = "Time",value.name = 'TT')
head(I_94_EB_TT_melt)
```


```{r I_94_EB_TT add_DateTime_I-94_EB}
I_94_EB_TT_melt <-  I_94_EB_TT_melt %>%
  
  ##we create a true datetime by merging the first day in the filename with the time from the column header.
  ##we make ensure it is a POSIX datetime format. 
  mutate(DateTime = as.POSIXct(paste(substr(gsub(".*\\_(.*)-.*","\\1",filename),1,8),
                                     str_sub(Time,-8,-1)),
                                     format="%Y%m%d %H:%M:%S"))

```

```{r I_94_EB_TT drop_fields}

##drop the no longer needed columns.
I_94_EB_TT_melt <- I_94_EB_TT_melt %>%
  select(-c(Average))

##show the top of the dataset.
head(I_94_EB_TT_melt)[1:5,1:length(I_94_EB_TT_melt)]  


```


We now join the Measure_Round value from the Linear Referenced Dataset in ArcMap to each station in our current dataset.

```{r I_94_EB_TT join_Measure_round}

##perform a join on our I_94_EB_TT_melt dataset with the Station table to get the Measure values for each station. 
I_94_EB_TT_melt <- merge(I_94_EB_TT_melt, Station[,c("Station", "Measure_Round")], by.x= "STN_ID", by.y="Station",all.x=TRUE )%>%arrange(DateTime)

I_94_EB_TT_melt$TT <- as.numeric(I_94_EB_TT_melt$TT)


##look at the top of the merged dataset
head(I_94_EB_TT_melt,n=50)

tail(I_94_EB_TT_melt,n=15)

##check if there were any location with no matches
print(table(is.na(I_94_EB_TT_melt$Measure)))

```

The Measure field has alot of significant digits. The Measure_Round field is the Measure field round to 2 decimal places in ArcMap. 



###I-94 Westbound Travel Time

We will focus on the Travel Time data for I_94 WB.


```{r I_94_WB_TT  read_bind,warning=FALSE,echo=FALSE,include=FALSE}
### set the path for the I-94 EB data.
path<-'./TICAS/I-94 WB'

### list out all the csv files that start with TT_ and then have a number. This will avoid loading in the aggregated files.
list_csv <- dir(path=path,pattern = '^TT_\\d.')

##use lapply to read each file in the list, creating a large list object. 
###We set header to be false because as we established the headers cannot be trused for dates. This will also aid in combining the dataset later.
myfiles <- lapply(paste(path,'/',list_csv,sep=''),function(x)read.csv(x,header = FALSE,stringsAsFactors = FALSE))

## we not add a column to each list element for the corresponding filenames we loaded. 
##This will be used to create a true datetime attribute.
files <- mapply(cbind,myfiles,'filename' = list_csv,SIMPLIFY = F)

## bind all the elements of the list together.
I_94_WB_TT <- bind_rows(files)

## we need to get rid of the symbol in the station name column and turn it to NA.
symbol <- I_94_WB_TT$V1[4]

I_94_WB_TT$V1[I_94_WB_TT$V1==symbol] <- NA

x <- I_94_WB_TT
```

We now clean the dataset and assign appropriate column names. 

```{r I_94_WB_TT  clean_mutate}
###I_94_WB_TT <- x

### Give the dataset the column name from the first row. This is only temporary.
colnames(I_94_WB_TT) <- I_94_WB_TT[1,]
colnames(I_94_WB_TT)[1] <- "STN_Name"
colnames(I_94_WB_TT)[292] <- 'filename'
colnames(I_94_WB_TT) <-trimws(colnames(I_94_WB_TT))


##remove the first row as it has now been transferred to the column names. 
I_94_WB_TT <- I_94_WB_TT[-1,]

## use dplyr piping to:
I_94_WB_TT <-  I_94_WB_TT %>%
  
  ##remove all of the headers that were read in orginially
  filter(I_94_WB_TT$`Accumulated Distance` != " Accumulated Distance")%>%
  
  ##Create new fields:
  ###fill in the blank rows with the previous name
  mutate(Name = paste(na.locf(STN_Name,na.rm=F),`Accumulated Distance`,seq=''),
         ###extract the station name from each row. The station name is included in parenthesis. 
         STN = gsub(".*\\((.*)\\).*","\\1",STN_Name),
         STN_ID = gsub(".*\\((.*)\\).*","\\1",Name),
         ## get the total lane number from the end of the station string.
         Lane = as.numeric(gsub(".*\\) ([^.]).*","\\1",Name)))%>%
  ##move the new columns to the front of the dataset
  select(Name,STN,STN_Name,STN_ID,Lane,filename,Average,everything())

###change the end of the day to 24:00 so it goes to the start of the next day
colnames(I_94_WB_TT) <- sub('00:00:00$','24:00:00',colnames(I_94_WB_TT))
  
  
  

```

The dataset is still in the wide format:

```{r I_94_WB_TT  check_dataset}

head(I_94_WB_TT[1:5,1:10])
table(I_94_WB_TT$`Accumulated Distance` == "Accumulated Distance")
```

Lets turn get a Time and Travel Time field effectively turning the dataset into the 'long format'. 

The melt function from reshape2 works well for this purpose.

```{r melt_I94_WB_TT}
I_94_WB_TT_melt <- melt(I_94_WB_TT,id.vars = c(1:8),variable.name = "Time",value.name = 'TT')
head(I_94_WB_TT_melt)
```

```{r I_94_WB_TT add_DateTime}
I_94_WB_TT_melt <-  I_94_WB_TT_melt %>%
  
  ##we create a true datetime by merging the first day in the filename with the time from the column header.
  ##we make ensure it is a POSIX datetime format. 
  mutate(DateTime = as.POSIXct(paste(substr(gsub(".*\\_(.*)-.*","\\1",filename),1,8),
                                     str_sub(Time,-8,-1)),
                                     format="%Y%m%d %H:%M:%S"))
  
```

```{r I_94_WB_TT remove_unneeded_fields}

##drop the no longer needed columns.
I_94_WB_TT_melt <- I_94_WB_TT_melt %>% select(-c(Average)) 

##show the top of the dataset.
head(I_94_WB_TT_melt)[1:5,1:length(I_94_WB_TT_melt)]  


```

Load in the linear referenced Station values from the FileGeodatabase. The File Geodatabase is currently located on C Drive due to locking issue with File Geodatabases stored on the Network Drive. We will need the milepost for each station. We are importing the entire Station list and some will not have measure values. 


```{r I_94_WB_TT join_Measure_round}

##perform a join on our I_94_WB_TT_melt dataset with the Station table to get the Measure values for each station. 
I_94_WB_TT_melt <- merge(I_94_WB_TT_melt, Station[,c("Station", "Measure_Round")], by.x= "STN_ID", by.y="Station",all.x=TRUE )

I_94_WB_TT_melt$TT <- as.numeric(I_94_WB_TT_melt$TT)

##look at the top of the merged dataset
head(I_94_WB_TT_melt,n=50)

tail(I_94_WB_TT_melt,n=15)

##check if there were any location with no matches
table(is.na(I_94_WB_TT_melt$Measure))

```

The Measure field has alot of significant digits. The Measure_Round field is the Measure field round to 2 decimal places in ArcMap. 




## VMT

###VMT I-94 Westbound


```{r I_94_WB_VMT read_VMT_bind,warning=FALSE,echo=FALSE,include=FALSE}
### set the path for the I-94 EB data.
path<-'./TICAS/I-94 WB'

### list out all the csv files that start with TT_ and then have a number. This will avoid loading in the aggregated files.
list_csv <- dir(path=path,pattern = '^VMT_\\d.')

##use lapply to read each file in the list, creating a large list object. 
###We set header to be false because as we established the headers cannot be trused for dates. This will also aid in combining the dataset later.
myfiles <- lapply(paste(path,'/',list_csv,sep=''),function(x)read.csv(x,header = FALSE,stringsAsFactors = FALSE))

## we not add a column to each list element for the corresponding filenames we loaded. 
##This will be used to create a true datetime attribute.
files <- mapply(cbind,myfiles,'filename' = list_csv,SIMPLIFY = F)

## bind all the elements of the list together.
I_94_WB_VMT <- bind_rows(files)

## we need to get rid of the symbol in the station name column and turn it to NA.
symbol <- I_94_WB_VMT$V1[4]

I_94_WB_VMT$V1[I_94_WB_VMT$V1==symbol] <- NA

x <- I_94_WB_VMT
```

We now clean the dataset and assign appropriate column names. 

```{r I_94_WB_VMT  clean_mutate}
##I_94_WB_VMT <- x

### Give the dataset the column name from the first row. This is only temporary.
colnames(I_94_WB_VMT) <- I_94_WB_VMT[1,]
colnames(I_94_WB_VMT)[1] <- "STN_Name"
colnames(I_94_WB_VMT)[291] <- 'filename'
colnames(I_94_WB_VMT) <-trimws(colnames(I_94_WB_VMT))


##remove the first row as it has now been transferred to the column names. 
I_94_WB_VMT <- I_94_WB_VMT[-1,]

## use dplyr piping to:
I_94_WB_VMT <-  I_94_WB_VMT %>%
  
  ##remove all of the headers that were read in orginially
  filter(I_94_WB_VMT$`Accumulated Distance` != " Accumulated Distance")%>%
  
  ##Create new fields:
  ###fill in the blank rows with the previous name
  mutate(Name = paste(na.locf(STN_Name,na.rm=F),`Accumulated Distance`,seq=''),
         ###extract the station name from each row. The station name is included in parenthesis. 
         STN = gsub(".*\\((.*)\\).*","\\1",STN_Name),
         STN_ID = gsub(".*\\((.*)\\).*","\\1",Name),
         ## get the total lane number from the end of the station string.
         Lane = as.numeric(gsub(".*\\) ([^.]).*","\\1",Name)))%>%
  ##move the new columns to the front of the dataset
  select(Name,STN,STN_Name,STN_ID,Lane,filename,everything())%>%
  ##remove total row
  filter(!grepl('Total',Name))
  
###change the end of the day to 24:00 so it goes to the start of the next day
colnames(I_94_WB_VMT) <- sub('00:00:00$','24:00:00',colnames(I_94_WB_VMT))
  
  

```

The dataset is still in the wide format:

```{r I_94_WB_VMT check_dataset}

head(I_94_WB_VMT[1:5,1:10])
table(I_94_WB_VMT$`Accumulated Distance` == "Accumulated Distance")
```

Lets turn get a Time and Travel Time field effectively turning the dataset into the 'long format'. 

The melt function from reshape2 works well for this purpose.

```{r I_94_WB_VMT melt_I94_WB}
I_94_WB_VMT_melt <- melt(I_94_WB_VMT,id.vars = c(1:7),variable.name = "Time",value.name = 'VMT')
head(I_94_WB_VMT_melt)
```

```{r I_94_WB_VMT add_DateTime}
I_94_WB_VMT_melt <-  I_94_WB_VMT_melt %>%
  
  ##we create a true datetime by merging the first day in the filename with the time from the column header.
  ##we make ensure it is a POSIX datetime format. 
  mutate(DateTime = as.POSIXct(paste(substr(gsub(".*\\_(.*)-.*","\\1",filename),1,8),
                                     str_sub(Time,-8,-1)),
                                     format="%Y%m%d %H:%M:%S"))
  
```


```{r I_94_WB_VMT join_Measure_round}

##perform a join on our I_94_WB_VMT_melt dataset with the Station table to get the Measure values for each station. 
I_94_WB_VMT_melt <- merge(I_94_WB_VMT_melt, Station[,c("Station", "Measure_Round")], by.x= "STN_ID", by.y="Station",all.x=TRUE )

I_94_WB_VMT_melt$VMT <- as.numeric(I_94_WB_VMT_melt$VMT)

##look at the top of the merged dataset
head(I_94_WB_VMT_melt,n=50)

tail(I_94_WB_VMT_melt,n=15)

##check if there were any location with no matches
table(is.na(I_94_WB_VMT_melt$Measure))

```

The Measure field has alot of significant digits. The Measure_Round field is the Measure field round to 2 decimal places in ArcMap. 



###VMT I-94 Eastbound


```{r I_94_EB_VMT read_VMT_bind,warning=FALSE,echo=FALSE,include=FALSE}
### set the path for the I-94 EB data.
path<-'./TICAS/I-94 EB'

### list out all the csv files that start with TT_ and then have a number. This will avoid loading in the aggregated files.
list_csv <- dir(path=path,pattern = '^VMT_\\d.')

##use lapply to read each file in the list, creating a large list object. 
###We set header to be false because as we established the headers cannot be trused for dates. This will also aid in combining the dataset later.
myfiles <- lapply(paste(path,'/',list_csv,sep=''),function(x)read.csv(x,header = FALSE,stringsAsFactors = FALSE))

## we not add a column to each list element for the corresponding filenames we loaded. 
##This will be used to create a true datetime attribute.
files <- mapply(cbind,myfiles,'filename' = list_csv,SIMPLIFY = F)

## bind all the elements of the list together.
I_94_EB_VMT <- bind_rows(files)

## we need to get rid of the symbol in the station name column and turn it to NA.
symbol <- I_94_EB_VMT$V1[4]

I_94_EB_VMT$V1[I_94_EB_VMT$V1==symbol] <- NA

x <- I_94_EB_VMT
```

We now clean the dataset and assign appropriate column names. 

```{r I_94_EB_VMT clean_mutate}
###I_94_EB_VMT <- x

### Give the dataset the column name from the first row. This is only temporary.
colnames(I_94_EB_VMT) <- I_94_EB_VMT[1,]
colnames(I_94_EB_VMT)[1] <- "STN_Name"
colnames(I_94_EB_VMT)[291] <- 'filename'
colnames(I_94_EB_VMT) <-trimws(colnames(I_94_EB_VMT))


##remove the first row as it has now been transferred to the column names. 
I_94_EB_VMT <- I_94_EB_VMT[-1,]

## use dplyr piping to:
I_94_EB_VMT <-  I_94_EB_VMT %>%
  
  ##remove all of the headers that were read in orginially
  filter(I_94_EB_VMT$`Accumulated Distance` != " Accumulated Distance")%>%
  
  ##Create new fields:
  ###fill in the blank rows with the previous name
  mutate(Name = paste(na.locf(STN_Name,na.rm=F),`Accumulated Distance`,seq=''),
         ###extract the station name from each row. The station name is included in parenthesis. 
         STN = gsub(".*\\((.*)\\).*","\\1",STN_Name),
         STN_ID = gsub(".*\\((.*)\\).*","\\1",Name),
         ## get the total lane number from the end of the station string.
         Lane = as.numeric(gsub(".*\\) ([^.]).*","\\1",Name)))%>%
  ##move the new columns to the front of the dataset
  select(Name,STN,STN_Name,STN_ID,Lane,filename,everything())%>%
  ##remove total row
  filter(!grepl('Total',Name))


###change the end of the day to 24:00 so it goes to the start of the next day
colnames(I_94_EB_VMT) <- sub('00:00:00$','24:00:00',colnames(I_94_EB_VMT))

```

The dataset is still in the wide format:

```{r I_94_EB_VMT check_dataset}

head(I_94_EB_VMT[1:5,1:10])
table(I_94_EB_VMT$`Accumulated Distance` == "Accumulated Distance")
```

Lets turn get a Time and Travel Time field effectively turning the dataset into the 'long format'. 

The melt function from reshape2 works well for this purpose.

```{r I_94_EB_VMT melt_I94_WB}
I_94_EB_VMT_melt <- melt(I_94_EB_VMT,id.vars = c(1:7),variable.name = "Time",value.name = 'VMT')
head(I_94_EB_VMT_melt)
```

```{r I_94_EB_VMT add_DateTime}
I_94_EB_VMT_melt <-  I_94_EB_VMT_melt %>%
  
  ##we create a true datetime by merging the first day in the filename with the time from the column header.
  ##we make ensure it is a POSIX datetime format. 
  mutate(DateTime = as.POSIXct(paste(substr(gsub(".*\\_(.*)-.*","\\1",filename),1,8),
                                     str_sub(Time,-8,-1)),
                                     format="%Y%m%d %H:%M:%S"))
  
```



Load in the linear referenced Station values from the FileGeodatabase. The File Geodatabase is currently located on C Drive due to locking issue with File Geodatabases stored on the Network Drive. We will need the milepost for each station. We are importing the entire Station list and some will not have measure values. 


```{r I_94_EB_VMT join_Measure_round}

##perform a join on our I_94_EB_VMT_melt dataset with the Station table to get the Measure values for each station. 
I_94_EB_VMT_melt <- merge(I_94_EB_VMT_melt, Station[,c("Station", "Measure_Round")], by.x= "STN_ID", by.y="Station",all.x=TRUE )

I_94_EB_VMT_melt$VMT <- as.numeric(I_94_EB_VMT_melt$VMT)


##look at the top of the merged dataset
head(I_94_EB_VMT_melt,n=50)

tail(I_94_EB_VMT_melt,n=15)

##check if there were any location with no matches
table(is.na(I_94_EB_VMT_melt$Measure))

```

The Measure field has alot of significant digits. The Measure_Round field is the Measure field round to 2 decimal places in ArcMap. 




## Crash Data MNCMAT

Read the data from ArcMAP Geodatabase. There is a Measure_Round field that contains the linear referenced milepost that we will use to take to the TT and VMT datasets.

```{r read_MNCMAT_dataset_from_ARCMAP}
fgdb <- "C:/Users/dgallen/Desktop/Geos/MN Detectors/Congestion.gdb"
MNCMAT <- as.data.frame(readOGR(dsn=fgdb,layer="MNCMAT"))
```

Create a date time field by combined the Month, Day, Year and Time fields. 

```{r create_DateTime}

MNCMAT$Time <- if_else(MNCMAT$Time<60,paste('00',MNCMAT$Time,sep = ''),if_else(MNCMAT$Time<10,paste('000',MNCMAT$Time,sep = ''),as.character(MNCMAT$Time)))

MNCMAT$DateTime <- as.POSIXct(paste(MNCMAT$Year,'/',
                                    MNCMAT$Month,'/',
                                    MNCMAT$Day,' ',
                                    ##not all times are formatted as 0524. 0524 is listed as 524.
                                    ##use str_sub to extract the correct pars of time
                                    str_sub(MNCMAT$Time,end=-3),':',
                                    str_sub(MNCMAT$Time,-2,-1), sep=''),format='%Y/%m/%d %H:%M')

##ensure that all date were formatted correctly. We should have no NAs.
table(is.na(MNCMAT$DateTime))
```

We have two NA remaining in the DateTime field. 

```{r check_NAs}
MNCMAT[is.na(MNCMAT$DateTime),c("Month","Day","Time","Year","DateTime")]
```

The Time values of 9998 are clearly in error and we will omit these records. 
We will also filter out the values with no Measure value. These values are not in the scope of this project. 

```{r remove_NAs}
MNCMAT_final <- MNCMAT %>% 
    filter(!is.na(DateTime))%>%
           filter(!is.na(Measure_Round))%>%
  arrange(DateTime)
```


Create a duration for each incident.

K: Fatal                      180 mins
A: Incapacitating Injury      90 mins
B: Non-Incapacitating Injury  45 mins
C: Possible Injury            30 mins
N: Property Damage            30 mins
X: Unknown                    30 mins


```{r assign_duration}
MNCMAT_final$EndDateTime <- if_else(MNCMAT_final$Sev == 'A',MNCMAT_final$DateTime+90*60,
                             if_else(MNCMAT_final$Sev == 'B',MNCMAT_final$DateTime+45*60,
                             if_else(MNCMAT_final$Sev == 'C',MNCMAT_final$DateTime+30*60,
                             if_else(MNCMAT_final$Sev == 'N',MNCMAT_final$DateTime+30*60,
                             if_else(MNCMAT_final$Sev == 'K',MNCMAT_final$DateTime+180*60,
                              if_else(MNCMAT_final$Sev == 'X',MNCMAT_final$DateTime+30*60,
                                     MNCMAT_final$DateTime))))))

MNCMAT_final$interval <- interval(MNCMAT_final$DateTime,MNCMAT_final$EndDateTime)

head(MNCMAT_final)
```


## Crash Data IRIS

```{r read_IRIS_dataset_from_ARCMAP}
fgdb <- "C:/Users/dgallen/Desktop/Geos/MN Detectors/Congestion.gdb"
IRIS_2015 <- as.data.frame(readOGR(dsn=fgdb,layer="IRIS_2015"))
```

Create a DateTime field.

```{r IRIS create_DateTime}

IRIS_2015$DateTime <- as.POSIXct(IRIS_2015$event_date,format='%Y-%m-%d %H:%M:%S')

table(is.na(IRIS_2015$DateTime))
```

Investigate the NA DateTime value.

```{r IRIS look_at_NA}
IRIS_2015[is.na(IRIS_2015$DateTime),]
```

The row appears to be blank so will remove the record.

```{r IRIS remove_NA}
IRIS_2015 <- IRIS_2015[!is.na(IRIS_2015$DateTime),]
```

We will group by event_id and return the min and max time. This will provide us with the interval for each incident. 

```{r aggregate_by_event_name}

IRIS_2015_event <- IRIS_2015 %>% 
  mutate(Descr = as.factor(word(IRIS_2015$descriptio,-1)))%>%
  select(-descriptio,everything())%>%
  group_by(name,Descr)%>%
  summarise(Min_DateTime = min(DateTime),
            Max_DateTime = max(DateTime),
            Road = first(road),
            Measure_Round = min(Measure_Round),
            Direction = first(direction),
            Dup = any(duplicated(lat)))%>%
  filter(!is.na(Measure_Round))%>%

  ##tibble (dplyr dataframe type) do not like storing interval column.
  as.data.frame()


head(IRIS_2015_event)
table(IRIS_2015_event$Road %>% droplevels())
table(IRIS_2015_event$Direction %>% droplevels())
```

We now have a start and end time, Direction and Road for each event.


##Weather 2015

Read the already processed weather data.

```{r}

weather_MSP <- as.data.frame(readxl::read_xlsx('./Weather/MSP_Weather_2015.xlsx',sheet = 'export3_2015'))
colnames(weather_MSP) <- trimws(colnames(weather_MSP))
colnames(weather_MSP) <- gsub('([[:punct:]])|\\s+','_',colnames(weather_MSP))

###read::xlsx brings in time in UTC. change to us/central
weather_MSP$Starting_Time <- as.POSIXct(format(weather_MSP$Starting_Time),tz='US/Central')
weather_MSP$End_Time <- as.POSIXct(format(weather_MSP$End_Time),tz='US/Central')
```



##Filter Combine Segments

```{r load_segments}
##load segment data
fgdb <- "C:/Users/dgallen/Desktop/Geos/MN Detectors/Congestion.gdb"
Seg <- as.data.frame(readOGR(dsn=fgdb,layer="Segments"))
print(Seg)
```

###TT and VMT aggregating functions

```{r TT_filter}

TT <- function(data,Station_MP,bins){
  data%>%
  filter(Measure_Round==Station_MP)%>%
  filter(!is.na(DateTime))%>%
  mutate(cut_ceiling=ceiling_date(DateTime,bins),
         cut_floor=floor_date(DateTime,bins))%>%
  group_by(cut_ceiling)%>%
  summarise(TT_mean=mean(TT),
            cut_floor=min(cut_floor))%>%
  as.data.frame()
}
  
```

```{r VMT_filter}

VMT <- function(data, from, to, bins){
  data %>%
  filter(Measure_Round >= from & Measure_Round <= to)%>%
  filter(!is.na(DateTime))%>%
  mutate(cut_ceiling=ceiling_date(DateTime,bins),
         cut_floor=floor_date(DateTime,bins))%>%
  group_by(cut_ceiling)%>%
  summarise(VMT_total = if_else(any(VMT<0),0, sum(VMT)),
            cut_floor=min(cut_floor))%>%
  as.data.frame()
}
```

```{r 15_minute_seq_2015}

seq_15min_2015 <- data.frame(DateTime=seq(as.POSIXct("2015,01,01",format="%Y,%m,%d"),
                                          as.POSIXct("2016,01,01",format="%Y,%m,%d"),by="15 min"))
seq_15min_2015$lag <- lag(seq_15min_2015$DateTime,1)
```



### S_1 TH_252_SouthBound MP 3.55 - MP 1.95

```{r S_1}
S_1_merge <- seq_15min_2015

```

Does the 15 minute time bin coincide with any MNCMAT events?

```{r S_1_MNCMAT}
S_1_MNCMAT <- MNCMAT_final %>% filter(Measure_Round>=0.00 &Measure_Round<=1.95 & Direction =='252S') %>% as.data.frame()

S_1_merge$crash1 <- sapply(S_1_merge$DateTime, function(x) any(x < S_1_MNCMAT$EndDateTime & x > S_1_MNCMAT$DateTime))
S_1_merge$crash2 <- sapply(S_1_merge$lag, function(x) any(x < S_1_MNCMAT$EndDateTime & x > S_1_MNCMAT$DateTime))
S_1_merge$MNCMAT <- ifelse(S_1_merge$crash1==TRUE | S_1_merge$crash2==TRUE,TRUE,FALSE)
```

There is no IRIS data for TH 252.


```{r S_1 final_crash}
S_1_merge$Crash <- if_else(S_1_merge$MNCMAT==TRUE,1,0)
```

Does the 15 minute time bin coincide with any weather events?

```{r S_1_Weather}
S_1_merge$weather1 <- sapply(S_1_merge$DateTime, function(x) any(x < weather_MSP$End_Time &
                                                                   x > weather_MSP$Starting_Time))
S_1_merge$weather2 <- sapply(S_1_merge$cut_floor.x, function(x) any(x < weather_MSP$End_Time & 
                                                                      x > weather_MSP$Starting_Time))
S_1_merge$weather <- ifelse(S_1_merge$weather2==TRUE | S_1_merge$weather1==TRUE,1,0) 
```

```{r S_1_final}
S_1_final <- S_1_merge %>% select(DateTime,weather,Crash)

write.csv(S_1_final,'S_1_final.csv',na='Null',row.names = FALSE)
```


### S_2 TH_252_SouthBound MP 1.95 - MP 0.00

```{r S_2}
S_2_merge <- seq_15min_2015

```

Does the 15 minute time bin coincide with any MNCMAT events?

```{r S_2_MNCMAT}
S_2_MNCMAT <- MNCMAT_final %>% filter(Measure_Round>=0.00 &Measure_Round<=1.95 & Direction =='252S') %>% as.data.frame()

S_2_merge$crash1 <- sapply(S_2_merge$DateTime, function(x) any(x < S_2_MNCMAT$EndDateTime & x > S_2_MNCMAT$DateTime))
S_2_merge$crash2 <- sapply(S_2_merge$lag, function(x) any(x < S_2_MNCMAT$EndDateTime & x > S_2_MNCMAT$DateTime))
S_2_merge$MNCMAT <- ifelse(S_2_merge$crash1==TRUE | S_2_merge$crash2==TRUE,TRUE,FALSE)
```

There is no IRIS data for TH 252.


```{r S_2 final_crash}
S_2_merge$Crash <- if_else(S_2_merge$MNCMAT==TRUE,1,0)
```

Does the 15 minute time bin coincide with any weather events?

```{r S_2_Weather}
S_2_merge$weather1 <- sapply(S_2_merge$DateTime, function(x) any(x < weather_MSP$End_Time &
                                                                   x > weather_MSP$Starting_Time))
S_2_merge$weather2 <- sapply(S_2_merge$cut_floor.x, function(x) any(x < weather_MSP$End_Time & 
                                                                      x > weather_MSP$Starting_Time))
S_2_merge$weather <- ifelse(S_2_merge$weather2==TRUE | S_2_merge$weather1==TRUE,1,0) 
```

```{r S_2_final}
S_2_final <- S_2_merge %>% select(DateTime,weather,Crash)

write.csv(S_2_final,'S_2_final.csv',na='Null',row.names = FALSE)
```



### N_3 TH_252_SouthBound MP 0.00 - MP 1.93

```{r N_3}
N_3_merge <- seq_15min_2015
```

Does the 15 minute time bin coincide with any MNCMAT events?

```{r N_3_MNCMAT}
N_3_MNCMAT <- MNCMAT_final %>% filter(Measure_Round>=0.00 &Measure_Round<=1.93 & Direction =='252N') %>% as.data.frame()

N_3_merge$crash1 <- sapply(N_3_merge$DateTime, function(x) any(x < N_3_MNCMAT$EndDateTime & x > N_3_MNCMAT$DateTime))
N_3_merge$crash2 <- sapply(N_3_merge$lag, function(x) any(x < N_3_MNCMAT$EndDateTime & x > N_3_MNCMAT$DateTime))
N_3_merge$MNCMAT <- ifelse(N_3_merge$crash1==TRUE | N_3_merge$crash2==TRUE,TRUE,FALSE)
```

There is no IRIS data for TH 252.


```{r N_3 final_crash}
N_3_merge$Crash <- if_else(N_3_merge$MNCMAT==TRUE,1,0)
```

Does the 15 minute time bin coincide with any weather events?

```{r N_3_Weather}
N_3_merge$weather1 <- sapply(N_3_merge$DateTime, function(x) any(x < weather_MSP$End_Time &
                                                                   x > weather_MSP$Starting_Time))
N_3_merge$weather2 <- sapply(N_3_merge$cut_floor.x, function(x) any(x < weather_MSP$End_Time & 
                                                                      x > weather_MSP$Starting_Time))
N_3_merge$weather <- ifelse(N_3_merge$weather2==TRUE | N_3_merge$weather1==TRUE,1,0) 
```

```{r N_3_final}
N_3_final <- N_3_merge %>% select(DateTime,weather,Crash)

write.csv(N_3_final,'N_3_final.csv',na='Null',row.names = FALSE)
```



### N_4 TH_252_SouthBound MP 1.93 - MP 3.62

```{r N_4}
N_4_merge <- seq_15min_2015
```

Does the 15 minute time bin coincide with any MNCMAT events?

```{r N_4_MNCMAT}
N_4_MNCMAT <- MNCMAT_final %>% filter(Measure_Round>=1.93 &Measure_Round<=3.62 & Direction =='252N') %>% as.data.frame()

N_4_merge$crash1 <- sapply(N_4_merge$DateTime, function(x) any(x < N_4_MNCMAT$EndDateTime & x > N_4_MNCMAT$DateTime))
N_4_merge$crash2 <- sapply(N_4_merge$lag, function(x) any(x < N_4_MNCMAT$EndDateTime & x > N_4_MNCMAT$DateTime))
N_4_merge$MNCMAT <- ifelse(N_4_merge$crash1==TRUE | N_4_merge$crash2==TRUE,TRUE,FALSE)
```

There is no IRIS data for TH 252.


```{r N_4 final_crash}
N_4_merge$Crash <- if_else(N_4_merge$MNCMAT==TRUE,1,0)
```

Does the 15 minute time bin coincide with any weather events?

```{r N_4_Weather}
N_4_merge$weather1 <- sapply(N_4_merge$DateTime, function(x) any(x < weather_MSP$End_Time &
                                                                   x > weather_MSP$Starting_Time))
N_4_merge$weather2 <- sapply(N_4_merge$cut_floor.x, function(x) any(x < weather_MSP$End_Time & 
                                                                      x > weather_MSP$Starting_Time))
N_4_merge$weather <- ifelse(N_4_merge$weather2==TRUE | N_4_merge$weather1==TRUE,1,0) 
```

```{r N_4_final}
N_4_final <- N_4_merge %>% select(DateTime,weather,Crash)

write.csv(N_4_final,'N_4_final.csv',na='Null',row.names = FALSE)
```







### N_1 I-94_WestBound MP 230.69 - MP 228.51

Filter both the TT and VMT datasets

```{r N_1}
N_1_VMT <- VMT(I_94_WB_VMT_melt,228.51,230.69,'15 mins')
N_1_TT <- TT(I_94_WB_TT_melt,230.69,'15 mins')

N_1_merge <- merge(seq_15min_2015,N_1_TT,by.x="DateTime",by.y="cut_ceiling",all.x=TRUE)
N_1_merge <- merge(N_1_merge,N_1_VMT,by.x="DateTime",by.y="cut_ceiling",all.x=TRUE)
```


Does the 15 minute time bin coincide with any MNCMAT events?

```{r N_1_MNCMAT}
N_1_MNCMAT <- MNCMAT_final %>% filter(Measure_Round>=225.63 &Measure_Round<=228.51 & Direction =='94W') %>% as.data.frame()

N_1_merge$crash1 <- sapply(N_1_merge$DateTime, function(x) any(x < N_1_MNCMAT$EndDateTime & x > N_1_MNCMAT$DateTime))
N_1_merge$crash2 <- sapply(N_1_merge$lag, function(x) any(x < N_1_MNCMAT$EndDateTime & x > N_1_MNCMAT$DateTime))
N_1_merge$MNCMAT <- ifelse(N_1_merge$crash1==TRUE | N_1_merge$crash2==TRUE,TRUE,FALSE)
```

Does the 15 minute time bin coincide with any IRIS events?

```{r N_1_IRIS}
N_1_IRIS <- IRIS_2015_event %>% filter(Measure_Round >=225.63 &Measure_Round<=228.51 & Direction == ' WB')

##CRASH
N_1_merge$IRIS_CRASH1 <- sapply(N_1_merge$DateTime, function(x) any(x < N_1_IRIS$Max_DateTime[N_1_IRIS$Descr=='CRASH'] & x > N_1_IRIS$Min_DateTime[N_1_IRIS$Descr=='CRASH']))

N_1_merge$IRIS_CRASH2 <- sapply(N_1_merge$lag, function(x) any(x < N_1_IRIS$Max_DateTime[N_1_IRIS$Descr=='CRASH'] & x > N_1_IRIS$Min_DateTime[N_1_IRIS$Descr=='CRASH']))

N_1_merge$IRIS_CRASH <- ifelse(N_1_merge$IRIS_CRASH1==TRUE | N_1_merge$IRIS_CRASH2==TRUE,TRUE,FALSE) 

###INCIDENT
N_1_merge$IRIS_INCIDENT1 <- sapply(N_1_merge$DateTime, function(x) any(x < N_1_IRIS$Max_DateTime[N_1_IRIS$Descr=='STALL'|N_1_IRIS$Descr=='HAZARD'] & x > N_1_IRIS$Min_DateTime[N_1_IRIS$Descr=='STALL'|N_1_IRIS$Descr=='HAZARD']))

N_1_merge$IRIS_INCIDENT2 <- sapply(N_1_merge$lag, function(x) any(x < N_1_IRIS$Max_DateTime[N_1_IRIS$Descr=='STALL'|N_1_IRIS$Descr=='HAZARD'] & x > N_1_IRIS$Min_DateTime[N_1_IRIS$Descr=='STALL'|N_1_IRIS$Descr=='HAZARD']))

N_1_merge$INCIDENT <- ifelse(N_1_merge$IRIS_INCIDENT1==TRUE | N_1_merge$IRIS_INCIDENT2==TRUE,1,0) 

##ROADWORK
N_1_merge$IRIS_ROADWORK1 <- sapply(N_1_merge$DateTime, function(x) any(x < N_1_IRIS$Max_DateTime[N_1_IRIS$Descr=='ROADWORK'] & x > N_1_IRIS$Min_DateTime[N_1_IRIS$Descr=='ROADWORK']))

N_1_merge$IRIS_ROADWORK2 <- sapply(N_1_merge$lag, function(x) any(x < N_1_IRIS$Max_DateTime[N_1_IRIS$Descr=='ROADWORK'] & x > N_1_IRIS$Min_DateTime[N_1_IRIS$Descr=='ROADWORK']))

N_1_merge$ROADWORK <- ifelse(N_1_merge$IRIS_ROADWORK1==TRUE | N_1_merge$IRIS_ROADWORK2==TRUE,1,0)  
```

```{r N_1 final_crash}
N_1_merge$Crash <- if_else(N_1_merge$IRIS_CRASH==TRUE|N_1_merge$MNCMAT==TRUE,1,0)
```

Does the 15 minute time bin coincide with any weather events?

```{r N_1_Weather}
N_1_merge$weather1 <- sapply(N_1_merge$DateTime, function(x) any(x < weather_MSP$End_Time &
                                                                   x > weather_MSP$Starting_Time))
N_1_merge$weather2 <- sapply(N_1_merge$lag, function(x) any(x < weather_MSP$End_Time & 
                                                                      x > weather_MSP$Starting_Time))
N_1_merge$weather <- ifelse(N_1_merge$weather2==TRUE | N_1_merge$weather1==TRUE,1,0) 
```

```{r N_1_final}
N_1_final <- N_1_merge %>% select(DateTime,VMT_total,TT_mean,weather,Crash,INCIDENT,ROADWORK)
N_1_final$VMT_total[N_1_final$VMT_total == 0] <- NA
write.csv(N_1_final,'N_1_final.csv',na='Null',row.names = FALSE)
```




### N_2 I-94_WestBound MP 228.51 - MP 225.63

Filter both the TT and VMT datasets

```{r N_2}
N_2_VMT <- VMT(I_94_WB_VMT_melt,225.63,228.51,'15 mins')
N_2_TT <- TT(I_94_WB_TT_melt,228.51,'15 mins')

N_2_merge <- merge(seq_15min_2015,N_2_TT,by.x="DateTime",by.y="cut_ceiling",all.x=TRUE)
N_2_merge <- merge(N_2_merge,N_2_VMT,by.x="DateTime",by.y="cut_ceiling",all.x=TRUE)
```


Does the 15 minute time bin coincide with any MNCMAT events?

```{r N_2_MNCMAT}
N_2_MNCMAT <- MNCMAT_final %>% filter(Measure_Round>=225.63 &Measure_Round<=228.51 & Direction =='94W') %>% as.data.frame()

N_2_merge$crash1 <- sapply(N_2_merge$DateTime, function(x) any(x < N_2_MNCMAT$EndDateTime & x > N_2_MNCMAT$DateTime))
N_2_merge$crash2 <- sapply(N_2_merge$lag, function(x) any(x < N_2_MNCMAT$EndDateTime & x > N_2_MNCMAT$DateTime))
N_2_merge$MNCMAT <- ifelse(N_2_merge$crash1==TRUE | N_2_merge$crash2==TRUE,TRUE,FALSE)
```

Does the 15 minute time bin coincide with any IRIS events?

```{r N_2_IRIS}
N_2_IRIS <- IRIS_2015_event %>% filter(Measure_Round >=225.63 &Measure_Round<=228.51 & Direction == ' WB')

##CRASH
N_2_merge$IRIS_CRASH1 <- sapply(N_2_merge$DateTime, function(x) any(x < N_2_IRIS$Max_DateTime[N_2_IRIS$Descr=='CRASH'] & x > N_2_IRIS$Min_DateTime[N_2_IRIS$Descr=='CRASH']))

N_2_merge$IRIS_CRASH2 <- sapply(N_2_merge$lag, function(x) any(x < N_2_IRIS$Max_DateTime[N_2_IRIS$Descr=='CRASH'] & x > N_2_IRIS$Min_DateTime[N_2_IRIS$Descr=='CRASH']))

N_2_merge$IRIS_CRASH <- ifelse(N_2_merge$IRIS_CRASH1==TRUE | N_2_merge$IRIS_CRASH2==TRUE,TRUE,FALSE) 

###INCIDENT
N_2_merge$IRIS_INCIDENT1 <- sapply(N_2_merge$DateTime, function(x) any(x < N_2_IRIS$Max_DateTime[N_2_IRIS$Descr=='STALL'|N_2_IRIS$Descr=='HAZARD'] & x > N_2_IRIS$Min_DateTime[N_2_IRIS$Descr=='STALL'|N_2_IRIS$Descr=='HAZARD']))

N_2_merge$IRIS_INCIDENT2 <- sapply(N_2_merge$lag, function(x) any(x < N_2_IRIS$Max_DateTime[N_2_IRIS$Descr=='STALL'|N_2_IRIS$Descr=='HAZARD'] & x > N_2_IRIS$Min_DateTime[N_2_IRIS$Descr=='STALL'|N_2_IRIS$Descr=='HAZARD']))

N_2_merge$INCIDENT <- ifelse(N_2_merge$IRIS_INCIDENT1==TRUE | N_2_merge$IRIS_INCIDENT2==TRUE,1,0) 

##ROADWORK
N_2_merge$IRIS_ROADWORK1 <- sapply(N_2_merge$DateTime, function(x) any(x < N_2_IRIS$Max_DateTime[N_2_IRIS$Descr=='ROADWORK'] & x > N_2_IRIS$Min_DateTime[N_2_IRIS$Descr=='ROADWORK']))

N_2_merge$IRIS_ROADWORK2 <- sapply(N_2_merge$lag, function(x) any(x < N_2_IRIS$Max_DateTime[N_2_IRIS$Descr=='ROADWORK'] & x > N_2_IRIS$Min_DateTime[N_2_IRIS$Descr=='ROADWORK']))

N_2_merge$ROADWORK <- ifelse(N_2_merge$IRIS_ROADWORK1==TRUE | N_2_merge$IRIS_ROADWORK2==TRUE,1,0)  
```

```{r N_2 final_crash}
N_2_merge$Crash <- if_else(N_2_merge$IRIS_CRASH==TRUE|N_2_merge$MNCMAT==TRUE,1,0)
```

Does the 15 minute time bin coincide with any weather events?

```{r N_2_Weather}
N_2_merge$weather1 <- sapply(N_2_merge$DateTime, function(x) any(x < weather_MSP$End_Time &
                                                                   x > weather_MSP$Starting_Time))
N_2_merge$weather2 <- sapply(N_2_merge$lag, function(x) any(x < weather_MSP$End_Time & 
                                                                      x > weather_MSP$Starting_Time))
N_2_merge$weather <- ifelse(N_2_merge$weather2==TRUE | N_2_merge$weather1==TRUE,1,0) 
```

```{r N_2_final}
N_2_final <- N_2_merge %>% select(DateTime,VMT_total,TT_mean,weather,Crash,INCIDENT,ROADWORK)
N_2_final$VMT_total[N_2_final$VMT_total == 0] <- NA
write.csv(N_2_final,'N_2_final.csv',na='Null',row.names = FALSE)
```





### S_3 I-94_EastBound MP 225.63 - MP 228.52

Filter both the TT and VMT datasets

```{r S_3}
S_3_VMT <- VMT(I_94_EB_VMT_melt,225.63,228.52,'15 mins')
S_3_TT <- TT(I_94_EB_TT_melt,228.52,'15 mins')

S_3_merge <- merge(seq_15min_2015,S_3_TT,by.x="DateTime",by.y="cut_ceiling",all.x=TRUE)
S_3_merge <- merge(S_3_merge,S_3_VMT,by.x="DateTime",by.y="cut_ceiling",all.x=TRUE)
```


Does the 15 minute time bin coincide with any MNCMAT events?

```{r S_3_MNCMAT}
S_3_MNCMAT <- MNCMAT_final %>% filter(Measure_Round>=225.63 &Measure_Round<=228.51 & Direction =='94W') %>% as.data.frame()

S_3_merge$crash1 <- sapply(S_3_merge$DateTime, function(x) any(x < S_3_MNCMAT$EndDateTime & x > S_3_MNCMAT$DateTime))
S_3_merge$crash2 <- sapply(S_3_merge$lag, function(x) any(x < S_3_MNCMAT$EndDateTime & x > S_3_MNCMAT$DateTime))
S_3_merge$MNCMAT <- ifelse(S_3_merge$crash1==TRUE | S_3_merge$crash2==TRUE,TRUE,FALSE)
```

Does the 15 minute time bin coincide with any IRIS events?

```{r S_3_IRIS}
S_3_IRIS <- IRIS_2015_event %>% filter(Measure_Round >=225.63 &Measure_Round<=228.51 & Direction == ' WB')

##CRASH
S_3_merge$IRIS_CRASH1 <- sapply(S_3_merge$DateTime, function(x) any(x < S_3_IRIS$Max_DateTime[S_3_IRIS$Descr=='CRASH'] & x > S_3_IRIS$Min_DateTime[S_3_IRIS$Descr=='CRASH']))

S_3_merge$IRIS_CRASH2 <- sapply(S_3_merge$lag, function(x) any(x < S_3_IRIS$Max_DateTime[S_3_IRIS$Descr=='CRASH'] & x > S_3_IRIS$Min_DateTime[S_3_IRIS$Descr=='CRASH']))

S_3_merge$IRIS_CRASH <- ifelse(S_3_merge$IRIS_CRASH1==TRUE | S_3_merge$IRIS_CRASH2==TRUE,TRUE,FALSE) 

###INCIDENT
S_3_merge$IRIS_INCIDENT1 <- sapply(S_3_merge$DateTime, function(x) any(x < S_3_IRIS$Max_DateTime[S_3_IRIS$Descr=='STALL'|S_3_IRIS$Descr=='HAZARD'] & x > S_3_IRIS$Min_DateTime[S_3_IRIS$Descr=='STALL'|S_3_IRIS$Descr=='HAZARD']))

S_3_merge$IRIS_INCIDENT2 <- sapply(S_3_merge$lag, function(x) any(x < S_3_IRIS$Max_DateTime[S_3_IRIS$Descr=='STALL'|S_3_IRIS$Descr=='HAZARD'] & x > S_3_IRIS$Min_DateTime[S_3_IRIS$Descr=='STALL'|S_3_IRIS$Descr=='HAZARD']))

S_3_merge$INCIDENT <- ifelse(S_3_merge$IRIS_INCIDENT1==TRUE | S_3_merge$IRIS_INCIDENT2==TRUE,1,0) 

##ROADWORK
S_3_merge$IRIS_ROADWORK1 <- sapply(S_3_merge$DateTime, function(x) any(x < S_3_IRIS$Max_DateTime[S_3_IRIS$Descr=='ROADWORK'] & x > S_3_IRIS$Min_DateTime[S_3_IRIS$Descr=='ROADWORK']))

S_3_merge$IRIS_ROADWORK2 <- sapply(S_3_merge$lag, function(x) any(x < S_3_IRIS$Max_DateTime[S_3_IRIS$Descr=='ROADWORK'] & x > S_3_IRIS$Min_DateTime[S_3_IRIS$Descr=='ROADWORK']))

S_3_merge$ROADWORK <- ifelse(S_3_merge$IRIS_ROADWORK1==TRUE | S_3_merge$IRIS_ROADWORK2==TRUE,1,0)  
```

```{r S_3 final_crash}
S_3_merge$Crash <- if_else(S_3_merge$IRIS_CRASH==TRUE|S_3_merge$MNCMAT==TRUE,1,0)
```

Does the 15 minute time bin coincide with any weather events?

```{r S_3_Weather}
S_3_merge$weather1 <- sapply(S_3_merge$DateTime, function(x) any(x < weather_MSP$End_Time &
                                                                   x > weather_MSP$Starting_Time))
S_3_merge$weather2 <- sapply(S_3_merge$lag, function(x) any(x < weather_MSP$End_Time & 
                                                                      x > weather_MSP$Starting_Time))
S_3_merge$weather <- ifelse(S_3_merge$weather2==TRUE | S_3_merge$weather1==TRUE,1,0) 
```

```{r S_3_final}
S_3_final <- S_3_merge %>% select(DateTime,VMT_total,TT_mean,weather,Crash,INCIDENT,ROADWORK)
S_3_final$VMT_total[S_3_final$VMT_total == 0] <- NA
write.csv(S_3_final,'S_3_final.csv',na='Null',row.names = FALSE)
```





### S_4 I-94_EastBound MP 228.52 - MP 230.72

Filter both the TT and VMT datasets

```{r S_4}
S_4_VMT <- VMT(I_94_EB_VMT_melt,228.52,230.72,'15 mins')
S_4_TT <- TT(I_94_EB_TT_melt,230.72,'15 mins')

S_4_merge <- merge(seq_15min_2015,S_4_TT,by.x="DateTime",by.y="cut_ceiling",all.x=TRUE)
S_4_merge <- merge(S_4_merge,S_4_VMT,by.x="DateTime",by.y="cut_ceiling",all.x=TRUE)
```



Does the 15 minute time bin coincide with any MNCMAT events?

```{r S_4_MNCMAT}
S_4_MNCMAT <- MNCMAT_final %>% filter(Measure_Round>=225.63 &Measure_Round<=228.51 & Direction =='94W') %>% as.data.frame()

S_4_merge$crash1 <- sapply(S_4_merge$DateTime, function(x) any(x < S_4_MNCMAT$EndDateTime & x > S_4_MNCMAT$DateTime))
S_4_merge$crash2 <- sapply(S_4_merge$lag, function(x) any(x < S_4_MNCMAT$EndDateTime & x > S_4_MNCMAT$DateTime))
S_4_merge$MNCMAT <- ifelse(S_4_merge$crash1==TRUE | S_4_merge$crash2==TRUE,TRUE,FALSE)
```

Does the 15 minute time bin coincide with any IRIS events?

```{r S_4_IRIS}
S_4_IRIS <- IRIS_2015_event %>% filter(Measure_Round >=225.63 &Measure_Round<=228.51 & Direction == ' WB')

##CRASH
S_4_merge$IRIS_CRASH1 <- sapply(S_4_merge$DateTime, function(x) any(x < S_4_IRIS$Max_DateTime[S_4_IRIS$Descr=='CRASH'] & x > S_4_IRIS$Min_DateTime[S_4_IRIS$Descr=='CRASH']))

S_4_merge$IRIS_CRASH2 <- sapply(S_4_merge$lag, function(x) any(x < S_4_IRIS$Max_DateTime[S_4_IRIS$Descr=='CRASH'] & x > S_4_IRIS$Min_DateTime[S_4_IRIS$Descr=='CRASH']))

S_4_merge$IRIS_CRASH <- ifelse(S_4_merge$IRIS_CRASH1==TRUE | S_4_merge$IRIS_CRASH2==TRUE,TRUE,FALSE) 

###INCIDENT
S_4_merge$IRIS_INCIDENT1 <- sapply(S_4_merge$DateTime, function(x) any(x < S_4_IRIS$Max_DateTime[S_4_IRIS$Descr=='STALL'|S_4_IRIS$Descr=='HAZARD'] & x > S_4_IRIS$Min_DateTime[S_4_IRIS$Descr=='STALL'|S_4_IRIS$Descr=='HAZARD']))

S_4_merge$IRIS_INCIDENT2 <- sapply(S_4_merge$lag, function(x) any(x < S_4_IRIS$Max_DateTime[S_4_IRIS$Descr=='STALL'|S_4_IRIS$Descr=='HAZARD'] & x > S_4_IRIS$Min_DateTime[S_4_IRIS$Descr=='STALL'|S_4_IRIS$Descr=='HAZARD']))

S_4_merge$INCIDENT <- ifelse(S_4_merge$IRIS_INCIDENT1==TRUE | S_4_merge$IRIS_INCIDENT2==TRUE,1,0) 

##ROADWORK
S_4_merge$IRIS_ROADWORK1 <- sapply(S_4_merge$DateTime, function(x) any(x < S_4_IRIS$Max_DateTime[S_4_IRIS$Descr=='ROADWORK'] & x > S_4_IRIS$Min_DateTime[S_4_IRIS$Descr=='ROADWORK']))

S_4_merge$IRIS_ROADWORK2 <- sapply(S_4_merge$lag, function(x) any(x < S_4_IRIS$Max_DateTime[S_4_IRIS$Descr=='ROADWORK'] & x > S_4_IRIS$Min_DateTime[S_4_IRIS$Descr=='ROADWORK']))

S_4_merge$ROADWORK <- ifelse(S_4_merge$IRIS_ROADWORK1==TRUE | S_4_merge$IRIS_ROADWORK2==TRUE,1,0)  
```

```{r S_4 final_crash}
S_4_merge$Crash <- if_else(S_4_merge$IRIS_CRASH==TRUE|S_4_merge$MNCMAT==TRUE,1,0)
```

Does the 15 minute time bin coincide with any weather events?

```{r S_4_Weather}
S_4_merge$weather1 <- sapply(S_4_merge$DateTime, function(x) any(x < weather_MSP$End_Time &
                                                                   x > weather_MSP$Starting_Time))
S_4_merge$weather2 <- sapply(S_4_merge$lag, function(x) any(x < weather_MSP$End_Time & 
                                                                      x > weather_MSP$Starting_Time))
S_4_merge$weather <- ifelse(S_4_merge$weather2==TRUE | S_4_merge$weather1==TRUE,1,0) 
```

```{r S_4_final}
S_4_final <- S_4_merge %>% select(DateTime,VMT_total,TT_mean,weather,Crash,INCIDENT,ROADWORK)
S_4_final$VMT_total[S_4_final$VMT_total == 0] <- NA
write.csv(S_4_final,'S_4_final.csv',na='Null',row.names = FALSE)
```






###Summary for TT and VMT

```{r TT_summary_function}

TT_summary <- function(data,name){
  x <- data %>%
  group_by(filename)%>%
  summarise(Min=min(TT),
            Max=max(TT),
            Mean=mean(TT),
            Median=median(TT),
            SD=sd(TT),
            below_0_percent = sum(TT<0)/length(TT))
            
  
  write.csv(x,name)
  print(x)
}
```

```{r TT_summaries}
TT_summary(I_94_EB_TT_melt,"I_94_EB_TT_summary.csv")
TT_summary(I_94_WB_TT_melt,"I_94_WB_TT_summary.csv")
```

```{r VMT_summary_function}
VMT_summary <- function(data,name){
  x <- data %>%
  group_by(filename)%>%
  mutate(VMT = as.numeric(VMT))%>%
  summarise(Min=min(VMT),
            Max=max(VMT),
            Mean=mean(VMT),
            Median=median(VMT),
            SD=sd(VMT),
            below_0_percent = sum(VMT<0)/length(VMT))
  
  write.csv(x,name)
  print(x)
}

```

```{r VMT_summaries}
VMT_summary(I_94_EB_VMT_melt,"I_94_EB_VMT_summary.csv")
VMT_summary(I_94_WB_VMT_melt,"I_94_WB_VMT_summary.csv")
```



```{r}
###boxplot(MNCMAT_final$Month~MNCMAT_final$Sev)

###stripchart(MNCMAT_final$Month~MNCMAT_final$Sev, vertical = TRUE, 
###           method = "jitter", add = TRUE, pch = 20, col = 'blue',offset=10)
```
---
title: "Reliability"
output: html_notebook
---

```{r setup,warning=FALSE}
knitr::opts_knit$set(root.dir = 'H:/Projects/11000/11155/TraffStudy/Reliability')

##below are a list of packages required to run the markdown file
library(tidyverse)
library(lubridate)
library(gridExtra)
library(reshape2)
library(zoo)
library(rgdal)
library(leaflet)
library(stringr)
```


We set the working directroy for the data files in the code above:
'H:/Projects/10000/10200/TS/Reliability/Data'

Here are the sub-folders at this Network Location. 

```{r dir_list}
data.frame(file.info(dir()))
```

##Load Station Linear References

Load in the linear referenced Station values from the FileGeodatabase. The File Geodatabase is currently located on a C Drive due to locking issue with File Geodatabases stored on the Network Drive.

MNDOT have a linear referenced network of roadways. These roadways are represented by polylines in ArcGIS and each polyline has a Milepost value that increases/decreases as you traverse the line. This allows any GPS point that lies close to any specific polyline to be "placed along the route" (ArcMap function name) and then given a Milepost value.

We used the 2016 Linear referencing dataset to give each Station and Detector location a relevant MP determined by the Roadway they are assoicated with.

We will need the milepost for each station. This will allow us to filter and segment crash data (which has GPS coordinates) while also filtering Station and Detector Data.

Caution: the Milepost values travel in one direction per corridor. For instance EB on I-94 in increasing MP whereas WB on I-94 is decreasing.

We are importing the entire Station list and some will not have measure values yet. 

```{r I_94_EB_TT connect_geodb_read_station}
### The location of the file geodatabase on the relevant C Drive
fgdb <- "C:/Users/dgallen/Desktop/Geos/MN Detectors/Congestion.gdb"
Station <- as.data.frame(readOGR(dsn=fgdb,layer="Station"))

##filter all stations that have been Linear Referenced.
Station <- Station[!is.na(Station$Measure) & Station$Roadway == 'I-494',] %>% droplevels()

###Some exploration on the Station Dataset.
head(Station)
class(Station)
colnames(Station)
```

## Travel Time

###494 Eastbound Travel Time

We will focus on the Travel Time data for 494 EB first. 


To load the Travel Time Data we first check that the files are present in the relevant folder. There are `r length(dir('./494 EB/TT'))` located in "./TICAS/I-94 EB".


The top ten files in the folder are shown here:
```{r I_94_EB_TT Top_10_Files_dir}
dir('./494 EB/TT')[1:10]
```

Each file is for a specific day, with the file name giving the actual day. 

Below we will investigate one of the files (top 5 rows, first 5 columns):


```{r 494_EB_TT sample_TT_data}
head(read.csv('./494 EB/TT/TT_201701040000-201701050000.csv')[1:5,1:5])
```

Data is not in a tidy format. Rows do not appear to have names and the data is in a 'wide' format as opposed to the preferred 'long' format i.e each column is a new time. We would prefer a column that is labelled 'Time' with each Station repeated as rows. Also from opening some of the csv files manually using Excel we notice that the date portion of the column headers are rarely correct. 

For example, the file loaded above states that the data is for Jan 4th to Feb 5th, but the column headers still state Jan 1st. However, the time seem to be correct and are incrementing in 5 minutes intervals.

We will choose to ignore the column header to identify dates and we will import the filenames to give the true date reading. We will still use the time portion of the column headers.

We will also fill down each row name to ensure that there are no blank station names. we will concatenate the accumulated distance with teh Station name to avoid repeated identical row names.  

We will load the data into R and combine into one dataset. 

```{r 494_EB_TT read_TT_files_bind,warning=FALSE}
### set the path for the I-94 EB data.
path <- './494 EB/TT'

### list out all the csv files that start with TT_ and then have a number. This will avoid loading in the unwanted aggregated files or VMT files.
###'^TT_\\d.' returns all files that start with TT_ and then are followed by a number.
list_csv <- dir(path=path,pattern = '^TT_\\d.')

##use lapply to read each file in the list, creating a large list object. 
###We set header to be false because as we established the headers cannot be trused for dates. This will also aid in combining the dataset later.
###column names will be defaulted V1 - Vn
myfiles <- lapply(paste(path,'/',list_csv,sep=''),function(x)read.csv(x,header = FALSE,stringsAsFactors = FALSE))

## we now add a column to each list element for the corresponding filenames we loaded. 
##This will be used to create a true datetime attribute.
files <- mapply(cbind,myfiles,'filename' = list_csv,SIMPLIFY = F)

## bind all the elements of the list together. This will convert the large list object to a dataframe. 
EB_494_TT_data <- bind_rows(files)

## we need to get rid of the symbol that is between each station name and turn it to NA.
symbol <- EB_494_TT_data$V1[4]

EB_494_TT_data$V1[EB_494_TT_data$V1==symbol] <- NA
```

We now clean the dataset and assign appropriate column names. 

```{r EB_494_TT fix_colnames_add_fields}

### Give the dataset the column name from the first row. We will only use the time portion of this first row. 
colnames(EB_494_TT_data) <- EB_494_TT_data[1,]
colnames(EB_494_TT_data)[1] <- "STN_Name"

##The last column will be the filenames
colnames(EB_494_TT_data)[length(EB_494_TT_data)] <- 'Filename'

#remove leading and trailing spaces in the column names. 
colnames(EB_494_TT_data) <-trimws(colnames(EB_494_TT_data))
colnames(EB_494_TT_data) <- str_replace_all(colnames(EB_494_TT_data),"[[:punct:]|\\s]+","_")

##remove the first row as it has now been transferred to the column names. 
EB_494_TT_data <- EB_494_TT_data[-1,]

## use dplyr piping to:
EB_494_TT <-  EB_494_TT_data %>%
  ##remove all of the headers that we ignored and that were read in orginially. These are identified by the rows which contain Accumulated Distance, an initial header name. 
  filter(Accumulated_Distance != " Accumulated Distance")%>%

  
##Create new fields:
###fill in the blank Station Name rows with the previous name and concatentate the accumulated Distance. 
mutate(Accumulated_Distance = as.numeric(trimws(Accumulated_Distance)),
       Name = paste(na.locf(STN_Name,na.rm=F),Accumulated_Distance,seq=''),
       ##extract the station name from each row. The station name is included in parenthesis. 
       STN = gsub(".*\\((.*)\\).*","\\1",STN_Name),
       STN_ID = gsub(".*\\((.*)\\).*","\\1",Name),
       ## get the total lane number from the end of the station string.
       Lane = as.numeric(gsub(".*\\) ([^.]).*","\\1",Name)))%>%
  ##move the new columns to the front of the dataset
  select(Name,STN,STN_Name,STN_ID,Lane,Filename,Average,everything())

###change the end of the day to 24:00 so it goes to the start of the next day
colnames(EB_494_TT) <- sub('00:00:00$','24:00:00',colnames(EB_494_TT))


```

The dataset is still in the wide format:

```{r EB_494_TT check_data_for_header_filter}
head(EB_494_TT[1:5,1:10])
##check to make sure that there are no header rows remaining. 
table(EB_494_TT$Accumulated_Distance == "Accumulated Distance")
```

We 'melt' the dataframe to obtain a Time and Travel Time field, effectively changing the dataset from 'wide' to the 'long format'. 

The melt function from reshape2 works well for this purpose.

```{r EB_494_TT melt}
EB_494_TT_melt <- melt(EB_494_TT,id.vars = c(1:8),variable.name = "Time",value.name = 'TT')
head(EB_494_TT_melt)
```

We need to create an appropriate DateTime column from a combination of the Time column headers and the filename. 

```{r EB_494_TT_melt add_DateTime}
EB_494_TT_melt <-  EB_494_TT_melt %>%
  
  ##we create a true datetime by merging the first day in the filename with the time from the column header.
  ##we make ensure it is a POSIX datetime format. 
  mutate(DateTime = as.POSIXct(paste(substr(gsub(".*\\_(.*)-.*","\\1",Filename),1,8),
                                     str_sub(Time,-8,-1)),
                                     format="%Y%m%d %H_%M_%S"))

```

We don't need the Average column so we remove.

```{r EB_494_TT_melt drop_fields}

##drop the no longer needed columns.
EB_494_TT_melt <- EB_494_TT_melt %>%
  select(-c(Average))

##show the top of the dataset.
head(EB_494_TT_melt)[1:5,1:length(EB_494_TT_melt)]  


```


We now 'join' the Measure_Round value from the Linear Referenced Dataset in ArcMap to each station in our current dataset.

The Measure field created by Linear Referencing has alot of significant digits. The Measure_Round field is the Measure field rounded to 2 decimal places in ArcMap. Measure_Round is the value we will reference.

```{r EB_494_TT_melt join_Measure_round}

##perform a join on our I_94_EB_TT_melt dataset with the Station table to get the Measure values for each station. 
EB_494_TT_melt <- merge(EB_494_TT_melt, Station[,c("Station", "Measure_Round")], by.x= "STN_ID", by.y="Station",all.x=TRUE )%>%arrange(DateTime)

###TT field was read in intially as a character field. change to numneric.
EB_494_TT_melt$TT <- as.numeric(EB_494_TT_melt$TT)

##look at the top of the merged dataset
head(EB_494_TT_melt,n=15)

tail(EB_494_TT_melt,n=15)

##check if there were any location with no matches
print(table(is.na(EB_494_TT_melt$Measure)))

```





###494 Westbound Travel Time

We will focus on the Travel Time data for I_94 WB.

The process here is the same as used in the Eastbound portion. All of the same comments may not be present here. 

Each file is for a specific day, with the file name giving the actual day. 

Below we will investigate one of the files (top 5 rows, first 5 columns):


```{r WB_494_TT  read_bind,warning=FALSE,echo=FALSE,include=FALSE}
### set the path for the I-94 EB data.
path <- './494 WB/TT'

### list out all the csv files that start with TT_ and then have a number. This will avoid loading in the aggregated files.
list_csv <- dir(path=path,pattern = '^TT_\\d.')

##use lapply to read each file in the list, creating a large list object. 
###We set header to be false because as we established the headers cannot be trused for dates. This will also aid in combining the dataset later.
myfiles <- lapply(paste(path,'/',list_csv,sep=''),function(x)read.csv(x,header = FALSE,stringsAsFactors = FALSE))

## we not add a column to each list element for the corresponding filenames we loaded. 
##This will be used to create a true datetime attribute.
files <- mapply(cbind,myfiles,'filename' = list_csv,SIMPLIFY = F)

## bind all the elements of the list together.
WB_494_TT_data <- bind_rows(files)

## we need to get rid of the symbol in the station name column and turn it to NA.
symbol <- WB_494_TT_data$V1[4]

WB_494_TT_data$V1[WB_494_TT_data$V1==symbol] <- NA


```

We now clean the dataset and assign appropriate column names. 

```{r WB_494_TT  clean_mutate}

colnames(WB_494_TT_data) <- WB_494_TT_data[1,]
colnames(WB_494_TT_data)[1] <- "STN_Name"

##The last column will be the filenames
colnames(WB_494_TT_data)[length(WB_494_TT_data)] <- 'Filename'

#remove leading and trailing spaces in the column names. 
colnames(WB_494_TT_data) <-trimws(colnames(WB_494_TT_data))
colnames(WB_494_TT_data) <- str_replace_all(colnames(WB_494_TT_data),"[[:punct:]|\\s]+","_")

##remove the first row as it has now been transferred to the column names. 
WB_494_TT_data <- WB_494_TT_data[-1,]

## use dplyr piping to:
WB_494_TT <-  WB_494_TT_data %>%
  ##remove all of the headers that we ignored and that were read in orginially. These are identified by the rows which contain Accumulated Distance, an initial header name. 
  filter(Accumulated_Distance != " Accumulated Distance")%>%
  
##Create new fields:
###fill in the blank Station Name rows with the previous name and concatentate the accumulated Distance. 
mutate(Accumulated_Distance = as.numeric(trimws(Accumulated_Distance)),
       Name = paste(na.locf(STN_Name,na.rm=F),Accumulated_Distance,seq=''),
       ##extract the station name from each row. The station name is included in parenthesis. 
       STN = gsub(".*\\((.*)\\).*","\\1",STN_Name),
       STN_ID = gsub(".*\\((.*)\\).*","\\1",Name),
       ## get the total lane number from the end of the station string.
       Lane = as.numeric(gsub(".*\\) ([^.]).*","\\1",Name)))%>%
  ##move the new columns to the front of the dataset
  select(Name,STN,STN_Name,STN_ID,Lane,Filename,Average,everything())
  
###change the end of the day to 24:00 so it goes to the start of the next day
colnames(WB_494_TT) <- sub('00:00:00$','24:00:00',colnames(WB_494_TT))
  
  
  

```

The dataset is still in the wide format:

```{r WB_494_TT  check_dataset}

head(WB_494_TT[1:5,1:10])
table(WB_494_TT$Accumulated_Distance == "Accumulated Distance")
```

We 'melt' the dataframe to obtain a Time and Travel Time field, effectively changing the dataset from 'wide' to the 'long format'. 

The melt function from reshape2 works well for this purpose.

```{r melt WB_494_TT}
WB_494_TT_melt <- melt(WB_494_TT,id.vars = c(1:8),variable.name = "Time",value.name = 'TT')
head(WB_494_TT_melt)
```

We need to create an appropriate DateTime column from a combination of the Time column headers and the filename. 

```{r WB_494_TT_melt add_DateTime}
WB_494_TT_melt <-  WB_494_TT_melt %>%
  
  ##we create a true datetime by merging the first day in the filename with the time from the column header.
  ##we make ensure it is a POSIX datetime format. 
  mutate(DateTime = as.POSIXct(paste(substr(gsub(".*\\_(.*)-.*","\\1",Filename),1,8),
                                     str_sub(Time,-8,-1)),
                                     format="%Y%m%d %H_%M_%S"))
  
```

We don't need the Average column so we remove.

```{r WB_494_TT_melt remove_unneeded_fields}

##drop the no longer needed columns.
WB_494_TT_melt <- WB_494_TT_melt %>% select(-c(Average)) 

##show the top of the dataset.
head(WB_494_TT_melt)[1:5,1:length(WB_494_TT_melt)]  


```

We now 'join' the Measure_Round value from the Linear Referenced Dataset in ArcMap to each station in our current dataset.

The Measure field created by Linear Referencing has alot of significant digits. The Measure_Round field is the Measure field rounded to 2 decimal places in ArcMap. Measure_Round is the value we will reference.


```{r I_94_WB_TT join_Measure_round}

##perform a join on our I_94_WB_TT_melt dataset with the Station table to get the Measure values for each station. 
WB_494_TT_melt <- merge(WB_494_TT_melt, Station[,c("Station", "Measure_Round")], by.x= "STN_ID", by.y="Station",all.x=TRUE )

WB_494_TT_melt$TT <- as.numeric(WB_494_TT_melt$TT)

##look at the top of the merged dataset
head(WB_494_TT_melt,n=50)

tail(WB_494_TT_melt,n=15)

##check if there were any location with no matches
table(is.na(WB_494_TT_melt$Measure))

```






## VMT

###VMT 494 Westbound


Each file is for a specific day, with the file name giving the actual day. 

Below we will investigate one of the files (top 5 rows, first 5 columns):

```{r WB_494_VMT read_VMT_bind,warning=FALSE,echo=FALSE,include=FALSE}
### set the path for the I-94 EB data.
path <- './494 WB/VMT'

### list out all the csv files that start with TT_ and then have a number. This will avoid loading in the aggregated files.
list_csv <- dir(path=path,pattern = '^VMT_\\d.')

##use lapply to read each file in the list, creating a large list object. 
###We set header to be false because as we established the headers cannot be trused for dates. This will also aid in combining the dataset later.
myfiles <- lapply(paste(path,'/',list_csv,sep=''),function(x)read.csv(x,header = FALSE,stringsAsFactors = FALSE))

## we not add a column to each list element for the corresponding filenames we loaded. 
##This will be used to create a true datetime attribute.
files <- mapply(cbind,myfiles,'Filename' = list_csv,SIMPLIFY = F)

## bind all the elements of the list together.
WB_494_VMT_data <- bind_rows(files)

## we need to get rid of the symbol in the station name column and turn it to NA.
symbol <- WB_494_VMT_data$V1[4]

WB_494_VMT_data$V1[WB_494_VMT_data$V1==symbol] <- NA


```

We now clean the dataset and assign appropriate column names. 

```{r WB_494_VMT_data  clean_mutate}

colnames(WB_494_VMT_data) <- WB_494_VMT_data[1,]
colnames(WB_494_VMT_data)[1] <- "STN_Name"

##The last column will be the filenames
colnames(WB_494_VMT_data)[length(WB_494_VMT_data)] <- 'Filename'

#remove leading and trailing spaces in the column names. 
colnames(WB_494_VMT_data) <-trimws(colnames(WB_494_VMT_data))
colnames(WB_494_VMT_data) <- str_replace_all(colnames(WB_494_VMT_data),"[[:punct:]|\\s]+","_")

##remove the first row as it has now been transferred to the column names. 
WB_494_VMT_data <- WB_494_VMT_data[-1,]

## use dplyr piping to:
WB_494_VMT <-  WB_494_VMT_data %>%
  ##remove all of the headers that we ignored and that were read in orginially. These are identified by the rows which contain Accumulated Distance, an initial header name. 
  filter(Accumulated_Distance != " Accumulated Distance")%>%
  
##Create new fields:
###fill in the blank Station Name rows with the previous name and concatentate the accumulated Distance. 
mutate(Name = paste(na.locf(STN_Name,na.rm=F),Accumulated_Distance,seq=''),
       ##extract the station name from each row. The station name is included in parenthesis. 
       STN = gsub(".*\\((.*)\\).*","\\1",STN_Name),
       STN_ID = gsub(".*\\((.*)\\).*","\\1",Name),
       ## get the total lane number from the end of the station string.
       Lane = as.numeric(gsub(".*\\) ([^.]).*","\\1",Name)))%>%
  ##move the new columns to the front of the dataset
  select(Name,STN,STN_Name,STN_ID,Lane,Filename,everything()) %>%
  filter(STN_ID != "Total  0 ")
  
###change the end of the day to 24:00 so it goes to the start of the next day
colnames(WB_494_VMT) <- sub('00:00:00$','24:00:00',colnames(WB_494_VMT))
  
  

```

The dataset is still in the wide format:

```{r WB_494_VMT check_dataset}

head(WB_494_VMT[1:5,1:10])
table(WB_494_VMT$Accumulated_Distance == "Accumulated Distance")
```

Lets turn get a Time and Travel Time field effectively turning the dataset into the 'long format'. 

The melt function from reshape2 works well for this purpose.

```{r WB_494_VMT melt_I94_WB}
WB_494_VMT_melt <- melt(WB_494_VMT,id.vars = c(1:7),variable.name = "Time",value.name = 'VMT')
head(WB_494_VMT_melt)
```
We need to create an appropriate DateTime column from a combination of the Time column headers and the filename. 

```{r WB_494_VMT_melt add_DateTime}
WB_494_VMT_melt <-  WB_494_VMT_melt %>%
  
  ##we create a true datetime by merging the first day in the filename with the time from the column header.
  ##we make ensure it is a POSIX datetime format. 
  mutate(DateTime = as.POSIXct(paste(substr(gsub(".*\\_(.*)-.*","\\1",Filename),1,8),
                                     str_sub(Time,-8,-1)),
                                     format="%Y%m%d %H_%M_%S"))
  
```

We now 'join' the Measure_Round value from the Linear Referenced Dataset in ArcMap to each station in our current dataset.

The Measure field created by Linear Referencing has alot of significant digits. The Measure_Round field is the Measure field rounded to 2 decimal places in ArcMap. Measure_Round is the value we will reference.

```{r I_94_WB_VMT join_Measure_round}

##perform a join on our I_94_WB_VMT_melt dataset with the Station table to get the Measure values for each station. 
WB_494_VMT_melt <- merge(WB_494_VMT_melt, Station[,c("Station", "Measure_Round")], by.x= "STN_ID", by.y="Station",all.x=TRUE )

WB_494_VMT_melt$VMT <- as.numeric(WB_494_VMT_melt$VMT)

##look at the top of the merged dataset
head(WB_494_VMT_melt,n=50)

tail(WB_494_VMT_melt,n=15)

##check if there were any location with no matches
table(is.na(WB_494_VMT_melt$Measure_Round))

```



###VMT 494 Eastbound


```{r EB_494_VMT read_VMT_bind,warning=FALSE,echo=FALSE,include=FALSE}
### set the path for the I-94 EB data.
path <- './494 EB/VMT'

### list out all the csv files that start with TT_ and then have a number. This will avoid loading in the aggregated files.
list_csv <- dir(path=path,pattern = '^VMT_\\d.')

##use lapply to read each file in the list, creating a large list object. 
###We set header to be false because as we established the headers cannot be trused for dates. This will also aid in combining the dataset later.
myfiles <- lapply(paste(path,'/',list_csv,sep=''),function(x)read.csv(x,header = FALSE,stringsAsFactors = FALSE))

## we not add a column to each list element for the corresponding filenames we loaded. 
##This will be used to create a true datetime attribute.
files <- mapply(cbind,myfiles,'filename' = list_csv,SIMPLIFY = F)

## bind all the elements of the list together.
EB_494_VMT_data <- bind_rows(files)

## we need to get rid of the symbol in the station name column and turn it to NA.
symbol <- EB_494_VMT_data$V1[4]

EB_494_VMT_data$V1[EB_494_VMT_data$V1==symbol] <- NA

```

We now clean the dataset and assign appropriate column names. 

```{r EB_494_VMT clean_mutate}

colnames(EB_494_VMT_data) <- EB_494_VMT_data[1,]
colnames(EB_494_VMT_data)[1] <- "STN_Name"

##The last column will be the filenames
colnames(EB_494_VMT_data)[length(EB_494_VMT_data)] <- 'Filename'

#remove leading and trailing spaces in the column names. 
colnames(EB_494_VMT_data) <-trimws(colnames(EB_494_VMT_data))
colnames(EB_494_VMT_data) <- str_replace_all(colnames(EB_494_VMT_data),"[[:punct:]|\\s]+","_")

##remove the first row as it has now been transferred to the column names. 
EB_494_VMT_data <- EB_494_VMT_data[-1,]

## use dplyr piping to:
EB_494_VMT <-  EB_494_VMT_data %>%
  ##remove all of the headers that we ignored and that were read in orginially. These are identified by the rows which contain Accumulated Distance, an initial header name. 
  filter(Accumulated_Distance != " Accumulated Distance")%>%
  
##Create new fields:
###fill in the blank Station Name rows with the previous name and concatentate the accumulated Distance. 
mutate(Name = paste(na.locf(STN_Name,na.rm=F),Accumulated_Distance,seq=''),
       ##extract the station name from each row. The station name is included in parenthesis. 
       STN = gsub(".*\\((.*)\\).*","\\1",STN_Name),
       STN_ID = gsub(".*\\((.*)\\).*","\\1",Name),
       ## get the total lane number from the end of the station string.
       Lane = as.numeric(gsub(".*\\) ([^.]).*","\\1",Name)))%>%
  ##move the new columns to the front of the dataset
  select(Name,STN,STN_Name,STN_ID,Lane,Filename,everything())%>%
  filter(STN_ID != "Total  0 ")
  
###change the end of the day to 24:00 so it goes to the start of the next day
colnames(EB_494_VMT) <- sub('00:00:00$','24:00:00',colnames(EB_494_VMT))

```

The dataset is still in the wide format:

```{r EB_494_VMT check_dataset}

head(EB_494_VMT[1:5,1:10])
table(EB_494_VMT$Accumulated_Distance == "Accumulated Distance")
```

Lets turn get a Time and Travel Time field effectively turning the dataset into the 'long format'. 

The melt function from reshape2 works well for this purpose.

```{r EB_494_VMT melt 494_EB}

EB_494_VMT_melt <- melt(EB_494_VMT,id.vars = c(1:7),variable.name = "Time",value.name = 'VMT')

head(EB_494_VMT_melt)
```

We need to create an appropriate DateTime column from a combination of the Time column headers and the filename. 

```{r EB_494_VMT_melt add_DateTime}
EB_494_VMT_melt <-  EB_494_VMT_melt %>%
  
  ##we create a true datetime by merging the first day in the filename with the time from the column header.
  ##we make ensure it is a POSIX datetime format. 
  mutate(DateTime = as.POSIXct(paste(substr(gsub(".*\\_(.*)-.*","\\1",Filename),1,8),
                                     str_sub(Time,-8,-1)),
                                     format="%Y%m%d %H_%M_%S"))
  
```

We now 'join' the Measure_Round value from the Linear Referenced Dataset in ArcMap to each station in our current dataset.

The Measure field created by Linear Referencing has alot of significant digits. The Measure_Round field is the Measure field rounded to 2 decimal places in ArcMap. Measure_Round is the value we will reference.


```{r I_94_EB_VMT join_Measure_round}

##perform a join on our I_94_EB_VMT_melt dataset with the Station table to get the Measure values for each station. 
EB_494_VMT_melt <- merge(EB_494_VMT_melt, Station[,c("Station", "Measure_Round")], by.x= "STN_ID", by.y="Station",all.x=TRUE )

EB_494_VMT_melt$VMT <- as.numeric(EB_494_VMT_melt$VMT)


##look at the top of the merged dataset
head(EB_494_VMT_melt,n=50)

tail(EB_494_VMT_melt,n=15)

##check if there were any location with no matches
table(is.na(EB_494_VMT_melt$Measure))

```




## Crash Data MNCMAT

Read the data from ArcMAP Geodatabase. There is a Measure_Round field that contains the linear referenced milepost that we will use to take to the TT and VMT datasets.

```{r read_MNCMAT_dataset_from_ARCMAP}
fgdb <- "C:/Users/dgallen/Desktop/Geos/MN Detectors/Congestion.gdb"

subset(ogrDrivers(), grepl("GDB", name))

fc_list = ogrListLayers(fgdb)

print(fc_list)

MNCMAT <- as.data.frame(readOGR(dsn=fgdb,layer="MNCMAT_2017")) %>% droplevels()
MNCMAT <- MNCMAT[!is.na(MNCMAT$Measure),] %>% droplevels()
```

Create a date time field by combined the Month, Day, Year and Time fields. 

```{r create_DateTime}

# MNCMAT$Time <- if_else(MNCMAT$Time<60,paste('00',MNCMAT$Time,sep = ''),if_else(MNCMAT$Time<10,paste('000',MNCMAT$Time,sep = ''),as.character(MNCMAT$Time)))
# 
# MNCMAT$DateTime <- as.POSIXct(paste(MNCMAT$Year,'/',
#                                     MNCMAT$Month,'/',
#                                     MNCMAT$Day,' ',
#                                     ##not all times are formatted as 0524. 0524 is listed as 524.
#                                     ##use str_sub to extract the correct pars of time
#                                     str_sub(MNCMAT$Time,end=-3),':',
#                                     str_sub(MNCMAT$Time,-2,-1), sep=''),format='%Y/%m/%d %H:%M')


MNCMAT$DateTime <- as.POSIXct(MNCMAT$DATE_TIME,format = "%Y/%m/%d %H:%M:%S")


##ensure that all date were formatted correctly. We should have no NAs.
table(is.na(MNCMAT$DateTime))
```


```{r arrange datetime}
MNCMAT_final <- MNCMAT %>% 
  arrange(DateTime)
```


Create a duration for each incident.

0: Fatal                      180 mins
1: Incapacitating Injury      90 mins
2: Non-Incapacitating Injury  45 mins
3: Possible Injury            30 mins
4: Property Damage            30 mins
5: Unknown                    30 mins


```{r assign_duration}
MNCMAT_final$EndDateTime <- if_else(MNCMAT_final$SEV == 0,MNCMAT_final$DateTime+18*60,
                             if_else(MNCMAT_final$SEV == 1,MNCMAT_final$DateTime+90*60,
                             if_else(MNCMAT_final$SEV == 2,MNCMAT_final$DateTime+45*60,
                             if_else(MNCMAT_final$SEV == 3,MNCMAT_final$DateTime+30*60,
                             if_else(MNCMAT_final$SEV == 4,MNCMAT_final$DateTime+30*60,
                              if_else(MNCMAT_final$SEV == 5,MNCMAT_final$DateTime+30*60,
                                     MNCMAT_final$DateTime))))))

MNCMAT_final$interval <- interval(MNCMAT_final$DateTime,MNCMAT_final$EndDateTime)

head(MNCMAT_final)
```


## Crash Data IRIS

Read the data from ArcMAP Geodatabase. There is a Measure_Round field that contains the linear referenced milepost that we will use to take to the TT and VMT datasets.

```{r read_IRIS_dataset_from_ARCMAP}
fgdb <- "C:/Users/dgallen/Desktop/Geos/MN Detectors/Congestion.gdb"
IRIS_2017 <- as.data.frame(readOGR(dsn=fgdb,layer="IRIS_2017"))
```

```{r IRIS}
y <-  read.csv('H:/Projects/11000/11155/TraffStudy/Reliability/Crash Data/IRIS Incidents 2017 (494 Confirmed).csv')
IRIS_2017 <- cbind(IRIS_2017,y) 
IRIS_2017 <- IRIS_2017 %>% select(colnames(IRIS_2017)[1:17])
colnames(IRIS_2017)[16:17] <- c('Start_Time', 'End_Time')
IRIS_2017 <- IRIS_2017[,-c(1,2)]
```



##Weather 2015

Read the already processed weather data.

```{r weather}
dir()
weather_MSP <- as.data.frame(readxl::read_xlsx('./Weather/Weather Data 2017.xlsx',sheet = 'Sheet7'))
colnames(weather_MSP) <- trimws(colnames(weather_MSP))
colnames(weather_MSP) <- gsub('([[:punct:]])|\\s+','_',colnames(weather_MSP))

###read::xlsx brings in time in UTC. change to us/central
weather_MSP$Start_Time <- as.POSIXct(format(weather_MSP$Start_Time),tz='US/Central')
weather_MSP$End_Time <- as.POSIXct(format(weather_MSP$End_Time),tz='US/Central')
```




## Show Data

```{r leaflet}

dat <- Station %>%
  filter(Direction == "WB")
m <- leaflet() %>%
  addTiles() %>%
  addMarkers(lng = dat$Longitude, lat = dat$Latitude,
             label = paste(dat$Direction, dat$Station,', MP: ', dat$Measure_Round,sep = ''))
m
```


##Filter Combine Segments

```{r load_segments}

## we linear referenced the segment designations. This table will not be used and is only for reference. n
##load segment data
fgdb <- "C:/Users/dgallen/Desktop/Geos/MN Detectors/Congestion.gdb"

subset(ogrDrivers(), grepl("GDB", name))

fc_list = ogrListLayers(fgdb)

print(fc_list)

Seg <- as.data.frame(readOGR(dsn=fgdb,layer="segment_494"))
print(Seg)
```

### Time Sequence

We create a vector of 15 min bins for 2017.

```{r 15_minute_seq_2017}

seq_15min_2017 <- data.frame(DateTime=seq(as.POSIXct("2017,01,01",format="%Y,%m,%d"),
                                          as.POSIXct("2018,01,01",format="%Y,%m,%d"),by="15 min"))
seq_15min_2017$lag <- lag(seq_15min_2017$DateTime,1)
```

### TT and VMT aggregating functions

```{r TT_filter}

##We create some custom functions that "Split Apply Combine" the TT datasets


##the TT function loads a clean TT dataset, filters for the segment bounds and aggregates into the time bins required.
##to find the final aggregated Travel Times,  the function takes the starting TT at the 'from' MP and subtracts it from the 'to' MP.
##the final number for the TT is the mean of all the TT in that time bin. 
TT <- function(data,from,to,bins){
data_from <-  data %>%
  filter(Measure_Round == from)%>%
  filter(!is.na(DateTime))%>%
  mutate(cut_ceiling=ceiling_date(DateTime,bins),
         cut_floor=floor_date(DateTime,bins))%>%
  group_by(cut_ceiling)%>%
  summarise(TT_mean_from=mean(TT),
            cut_floor=min(cut_floor))%>%
  mutate(YearDay = strftime(cut_ceiling, format="%D"),
         DayTime = strftime(cut_ceiling, format="%H:%M"))

data_to <-  data %>%
  filter(Measure_Round == to)%>%
  filter(!is.na(DateTime))%>%
  mutate(cut_ceiling=ceiling_date(DateTime,bins),
         cut_floor=floor_date(DateTime,bins))%>%
  group_by(cut_ceiling)%>%
  summarise(TT_mean_to=mean(TT),
            cut_floor=min(cut_floor))  
  
merge_data <-  merge(data_from,data_to,by='cut_ceiling',all.x=TRUE,all.y=TRUE)  %>%
  mutate(TT_mean= TT_mean_to-TT_mean_from)
return(merge_data)

}

```

```{r VMT_filter}

##We create some custom functions that "Split Apply Combine" the VMT datasets


##the VMT function loads a clean VMT dataset, filters for the segment bounds and aggregates into the time bins required.
##to find the final Vehicular Miles traveled,  the function takes the starting VMT at the 'from' MP and subtracts it from the 'to' MP.
##the final number for the VMT is the mean of all the VMT in that time bin. 


VMT <- function(data, from, to, bins){
data_from <-   data %>%
  filter(Measure_Round <= from )%>%
  filter(!is.na(DateTime))%>%
  mutate(cut_ceiling=ceiling_date(DateTime,bins),
         cut_floor=floor_date(DateTime,bins))%>%
  group_by(cut_ceiling)%>%
  summarise(VMT_total_from = if_else(any(VMT<0),0, sum(VMT)),
            cut_floor=min(cut_floor))%>%
  as.data.frame()
  
data_to <-  data %>%
  filter(Measure_Round <= to )%>%
  filter(!is.na(DateTime))%>%
  mutate(cut_ceiling=ceiling_date(DateTime,bins),
         cut_floor=floor_date(DateTime,bins))%>%
  group_by(cut_ceiling)%>%
  summarise(VMT_total_to = if_else(any(VMT<0),0, sum(VMT)),
            cut_floor=min(cut_floor))%>%
  as.data.frame()

merge_data <-  merge(data_from,data_to,by='cut_ceiling',all.x=TRUE,all.y=TRUE)  %>%
  mutate(VMT_total= VMT_total_to-VMT_total_from)
return(merge_data)

}


##The VMT_switch function is used when travel direction is opposite to the Milepost incrementing value.
VMT_switch <- function(data, from, to, bins){
data_from <-   data %>%
  filter(Measure_Round >= from )%>%
  filter(!is.na(DateTime))%>%
  mutate(cut_ceiling=ceiling_date(DateTime,bins),
         cut_floor=floor_date(DateTime,bins))%>%
  group_by(cut_ceiling)%>%
  summarise(VMT_total_from = if_else(any(VMT<0),0, sum(VMT)),
            cut_floor=min(cut_floor))%>%
  as.data.frame()
  
data_to <-  data %>%
  filter(Measure_Round >= to )%>%
  filter(!is.na(DateTime))%>%
  mutate(cut_ceiling=ceiling_date(DateTime,bins),
         cut_floor=floor_date(DateTime,bins))%>%
  group_by(cut_ceiling)%>%
  summarise(VMT_total_to = if_else(any(VMT<0),0, sum(VMT)),
            cut_floor=min(cut_floor))%>%
  as.data.frame()
  
merge_data <-  merge(data_from,data_to,by='cut_ceiling',all.x=TRUE,all.y=TRUE)  %>%
  mutate(VMT_total= VMT_total_to-VMT_total_from)
return(merge_data)
}

```

### Crash aggregating functions

```{r MNCMAT}

MNCMAT_seg <- function(Start,End,Direction,Crash_data,Time_seq) {
  
  merge_data <- Time_seq
  
  MNCMAT <- Crash_data %>% 
    filter(Measure_Round >= Start & Measure_Round <= End & Direction ==  Direction) %>% 
    as.data.frame()
  
  merge_data$crash1 <- sapply(merge_data$DateTime,
                              function(x) any(x < MNCMAT$EndDateTime & x > MNCMAT$DateTime))
  
  merge_data$crash2 <- sapply(merge_data$lag,
                              function(x) any(x < MNCMAT$EndDateTime & x > MNCMAT$DateTime))
  
  merge_data$MNCMAT <- ifelse(merge_data$crash1==TRUE | merge_data$crash2==TRUE,TRUE,FALSE)
  
  return(merge_data)
}

```

```{r IRIS}

IRIS_seg <- function(Start,End,Direction,Crash_data,Time_seq) {

  merge_data <- Time_seq
  
IRIS <- Crash_data %>% filter(Measure_Round >= Start & Measure_Round <= End & Direction == Direction)

##CRASH
merge_data$IRIS_CRASH1 <- sapply(merge_data$DateTime, function(x) any(x < IRIS$Max_DateTime[IRIS$Descr=='CRASH'] & x > IRIS$Min_DateTime[IRIS$Descr=='CRASH']))

merge_data$IRIS_CRASH2 <- sapply(merge_data$lag, function(x) any(x < IRIS$Max_DateTime[IRIS$Descr=='CRASH'] & x > IRIS$Min_DateTime[IRIS$Descr=='CRASH']))

merge_data$CRASH <- ifelse(merge_data$IRIS_CRASH1==TRUE | merge_data$IRIS_CRASH2==TRUE,TRUE,FALSE) 

###INCIDENT
merge_data$IRIS_INCIDENT1 <- sapply(merge_data$DateTime, function(x) any(x < IRIS$Max_DateTime[IRIS$Descr=='STALL'|IRIS$Descr=='HAZARD'] & x > IRIS$Min_DateTime[IRIS$Descr=='STALL'|IRIS$Descr=='HAZARD']))

merge_data$IRIS_INCIDENT2 <- sapply(merge_data$lag, function(x) any(x < IRIS$Max_DateTime[IRIS$Descr=='STALL'|IRIS$Descr=='HAZARD'] & x > IRIS$Min_DateTime[IRIS$Descr=='STALL'|IRIS$Descr=='HAZARD']))

merge_data$INCIDENT <- ifelse(merge_data$IRIS_INCIDENT1==TRUE | merge_data$IRIS_INCIDENT2==TRUE,1,0) 

##ROADWORK
merge_data$IRIS_ROADWORK1 <- sapply(merge_data$DateTime, function(x) any(x < IRIS$Max_DateTime[IRIS$Descr=='ROADWORK'] & x > IRIS$Min_DateTime[IRIS$Descr=='ROADWORK']))

merge_data$IRIS_ROADWORK2 <- sapply(merge_data$lag, function(x) any(x < IRIS$Max_DateTime[IRIS$Descr=='ROADWORK'] & x > IRIS$Min_DateTime[IRIS$Descr=='ROADWORK']))

merge_data$ROADWORK <- ifelse(merge_data$IRIS_ROADWORK1==TRUE | merge_data$IRIS_ROADWORK2==TRUE,1,0)  

return(merge_data)

}
```


### Weather aggregating function

```{r Weather}

weather <- function(weather_data,Time_seq){
  
  merge_data <- Time_seq
  
  merge_data$weather1 <- sapply(merge_data$DateTime,
                                function(x) any(x < weather_data$End_Time &
                                                                     x > weather_data$Starting_Time))
  
  merge_data$weather2 <- sapply(merge_data$cut_floor.x,
                               function(x) any(x < weather_data$End_Time & 
                                                                        x > weather_data$Starting_Time))
  
  merge_data$weather <- ifelse(merge_data$weather2==TRUE | S_1_merge$weather1==TRUE,1,0) 
  
  return(merge_data)
}
```




### SEG 6

```{r seg 6}

TT6 <- TT(data = WB_494_TT_melt ,from = 1.05,to = 3.3, bins = '15 mins')

TT6_wide <- TT6 %>% 
  select(YearDay,DayTime,TT_mean) %>% 
  dcast(DayTime ~ YearDay, value.var = 'TT_mean')

TT6_mat <- as.matrix(TT6_wide[,-1])
rownames(TT6_mat) <- TT6_wide[,1]

VMT6 <- VMT(data = WB_494_VMT_melt,from = 1.05, to = 3.3, bins = '15 mins')

```





```{r S_1_final}
S_1_final <- S_1_merge %>% select(DateTime,weather,Crash)

write.csv(S_1_final,'S_1_final.csv',na='Null',row.names = FALSE)
```


### N_1 I-94_WestBound MP 230.69 - MP 228.51

Filter both the TT and VMT datasets

```{r N_1}
N_1_VMT <- VMT_switch(I_94_WB_VMT_melt,230.69,228.51,'15 mins')
N_1_TT <- TT(I_94_WB_TT_melt,230.69,228.51,'15 mins')

N_1_merge <- merge(seq_15min_2015,N_1_TT,by.x="DateTime",by.y="cut_ceiling",all.x=TRUE)
N_1_merge <- merge(N_1_merge,N_1_VMT,by.x="DateTime",by.y="cut_ceiling",all.x=TRUE)
```


Does the 15 minute time bin coincide with any MNCMAT events?

```{r N_1_MNCMAT}
N_1_MNCMAT <- MNCMAT_final %>% filter(Measure_Round>=228.51 &Measure_Round<=230.69 & Direction =='94W') %>% as.data.frame()

N_1_merge$crash1 <- sapply(N_1_merge$DateTime, function(x) any(x < N_1_MNCMAT$EndDateTime & x > N_1_MNCMAT$DateTime))
N_1_merge$crash2 <- sapply(N_1_merge$lag, function(x) any(x < N_1_MNCMAT$EndDateTime & x > N_1_MNCMAT$DateTime))
N_1_merge$MNCMAT <- ifelse(N_1_merge$crash1==TRUE | N_1_merge$crash2==TRUE,TRUE,FALSE)
```

Does the 15 minute time bin coincide with any IRIS events?

```{r N_1_IRIS}
N_1_IRIS <- IRIS_2015_event %>% filter(Measure_Round >=228.51 &Measure_Round<=230.69 & Direction == ' WB')

##CRASH
N_1_merge$IRIS_CRASH1 <- sapply(N_1_merge$DateTime, function(x) any(x < N_1_IRIS$Max_DateTime[N_1_IRIS$Descr=='CRASH'] & x > N_1_IRIS$Min_DateTime[N_1_IRIS$Descr=='CRASH']))

N_1_merge$IRIS_CRASH2 <- sapply(N_1_merge$lag, function(x) any(x < N_1_IRIS$Max_DateTime[N_1_IRIS$Descr=='CRASH'] & x > N_1_IRIS$Min_DateTime[N_1_IRIS$Descr=='CRASH']))

N_1_merge$IRIS_CRASH <- ifelse(N_1_merge$IRIS_CRASH1==TRUE | N_1_merge$IRIS_CRASH2==TRUE,TRUE,FALSE) 

###INCIDENT
N_1_merge$IRIS_INCIDENT1 <- sapply(N_1_merge$DateTime, function(x) any(x < N_1_IRIS$Max_DateTime[N_1_IRIS$Descr=='STALL'|N_1_IRIS$Descr=='HAZARD'] & x > N_1_IRIS$Min_DateTime[N_1_IRIS$Descr=='STALL'|N_1_IRIS$Descr=='HAZARD']))

N_1_merge$IRIS_INCIDENT2 <- sapply(N_1_merge$lag, function(x) any(x < N_1_IRIS$Max_DateTime[N_1_IRIS$Descr=='STALL'|N_1_IRIS$Descr=='HAZARD'] & x > N_1_IRIS$Min_DateTime[N_1_IRIS$Descr=='STALL'|N_1_IRIS$Descr=='HAZARD']))

N_1_merge$INCIDENT <- ifelse(N_1_merge$IRIS_INCIDENT1==TRUE | N_1_merge$IRIS_INCIDENT2==TRUE,1,0) 

##ROADWORK
N_1_merge$IRIS_ROADWORK1 <- sapply(N_1_merge$DateTime, function(x) any(x < N_1_IRIS$Max_DateTime[N_1_IRIS$Descr=='ROADWORK'] & x > N_1_IRIS$Min_DateTime[N_1_IRIS$Descr=='ROADWORK']))

N_1_merge$IRIS_ROADWORK2 <- sapply(N_1_merge$lag, function(x) any(x < N_1_IRIS$Max_DateTime[N_1_IRIS$Descr=='ROADWORK'] & x > N_1_IRIS$Min_DateTime[N_1_IRIS$Descr=='ROADWORK']))

N_1_merge$ROADWORK <- ifelse(N_1_merge$IRIS_ROADWORK1==TRUE | N_1_merge$IRIS_ROADWORK2==TRUE,1,0)  
```

```{r N_1 final_crash}
N_1_merge$Crash <- if_else(N_1_merge$IRIS_CRASH==TRUE|N_1_merge$MNCMAT==TRUE,1,0)
```

Does the 15 minute time bin coincide with any weather events?

```{r N_1_Weather}
N_1_merge$weather1 <- sapply(N_1_merge$DateTime, function(x) any(x < weather_MSP$End_Time &
                                                                   x > weather_MSP$Starting_Time))
N_1_merge$weather2 <- sapply(N_1_merge$lag, function(x) any(x < weather_MSP$End_Time & 
                                                                      x > weather_MSP$Starting_Time))
N_1_merge$weather <- ifelse(N_1_merge$weather2==TRUE | N_1_merge$weather1==TRUE,1,0) 
```

```{r N_1_final}
N_1_final <- N_1_merge %>% select(DateTime,VMT_total,TT_mean,weather,Crash,INCIDENT,ROADWORK)
N_1_final$VMT_total[N_1_final$VMT_total == 0] <- NA
N_1_final$VMT_total[N_1_final$VMT_total < 0] <- NA
write.csv(N_1_final,'N_1_final_rev2.csv',na='Null',row.names = FALSE)
```





###Summary for TT and VMT

```{r TT_summary_function}

TT_summary <- function(data,name){
  x <- data %>%
  group_by(filename)%>%
  summarise(Min=min(TT),
            Max=max(TT),
            Mean=mean(TT),
            Median=median(TT),
            SD=sd(TT),
            below_0_percent = sum(TT<0)/length(TT))
            
  
  write.csv(x,name)
  print(x)
}
```

```{r TT_summaries}
TT_summary(I_94_EB_TT_melt,"I_94_EB_TT_summary.csv")
TT_summary(I_94_WB_TT_melt,"I_94_WB_TT_summary.csv")
```

```{r VMT_summary_function}
VMT_summary <- function(data,name){
  x <- data %>%
  group_by(filename)%>%
  mutate(VMT = as.numeric(VMT))%>%
  summarise(Min=min(VMT),
            Max=max(VMT),
            Mean=mean(VMT),
            Median=median(VMT),
            SD=sd(VMT),
            below_0_percent = sum(VMT<0)/length(VMT))
  
  write.csv(x,name)
  print(x)
}

```

```{r VMT_summaries}
VMT_summary(I_94_EB_VMT_melt,"I_94_EB_VMT_summary.csv")
VMT_summary(I_94_WB_VMT_melt,"I_94_WB_VMT_summary.csv")
```



```{r}
###boxplot(MNCMAT_final$Month~MNCMAT_final$Sev)

###stripchart(MNCMAT_final$Month~MNCMAT_final$Sev, vertical = TRUE, 
###           method = "jitter", add = TRUE, pch = 20, col = 'blue',offset=10)
```